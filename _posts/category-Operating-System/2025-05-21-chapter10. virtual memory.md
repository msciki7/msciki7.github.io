---
title: "Chapter 10. Virtual Memory"
excerpt: ""

wirter: sohee kim
categories:
  - Operating System
tags:
  - operating system

toc: true
use_math: true
toc_sticky: true
  
date: 2025-05-21
last_modified_at: 2025-05-21
---

1\. 배경
=======

&ensp;코드는 메모리에 있어야 실행될 수 있지만, 전체 프로그램이 항상 동시에 필요하지는 않는다. <br/>
* 일부 코드(예. 에러 처리, 비정상 루틴 등)만 사용될 수 있다.
* 그래서 프로그램 전체를 한꺼번에 메모리에 올릴 필요가 없다. 
* 부분적으로 로드된 프로그램 실행이 가능하다.

&ensp;그에 인한 효과<br/>
* 물리적 메모리의 제한에 덜 구속된다.
* 각 프로그램이 메모리를 적게 차지 → 더 많은 프로그램을 동시에 실행 가능하다.
* I/O 작업 감소 → 프로그램을 메모리에 로드하거나 스왑할 필요 줄어든다. 
* CPU 활용률 증가.

virtual Memory
------

&ensp;사용자 논리 주소 공간(logical memory space)을 물리 주소 공간(physical memory)과 분리하여 더 유연하게 메모리를 사용하는 방식이다. 

* 이점
    - 프로그램의 일부만 메모리에 있어도 실행 가능.
    - 논리 주소 공간은 물리 주소 공간보다 훨씬 클 수 있음 (예: 64비트 시스템에서 2^64까지).
    - 주소 공간 공유 가능: 여러 프로세스가 코드 영역 등을 공유할 수 있음.
    - 프로세스 생성 효율 향상: 복사보다 매핑이 빠름.
    - 동시 실행 프로그램 수 증가
    - I/O 감소: 로딩과 스왑 빈도 증가


Virtual Address Space
------

&ensp;논리적인 주소 공간으로 프로세스가 메모리를 바라보는 방식이다.<br/>

* 구성
  - 보통 0번 주소부터 연속된 공간으로 시작(실제로는 페이지 단위로 관리)
  - 물리 메모리는 페이지 프레임(frame) 단위로 나뉘며 MMU가 논리 주소를 물리 주소로 변환

* 가상 메모리 구현 방식
  - **Demand Paging(요구 페이징)**
    + 실제로 필요한 페이지만 로드하는 방식
    + 처음에는 아무것도 메모리에 올리지 않는다.
    + 접근이 발생할 때 페이지 폴트(Page Fault)가 발생하면 그때서야 디스크에서 해당 페이지를 로드한다. 

* 예시
&ensp;한 프로그램이 1GB이고, 실제 자주 사용하는 부분은 100MB라고 해보죠.

* 기존 방식
  - 1GB 전체를 메모리에 올려야 실행됨.

* 가상 메모리 + 요구페이징
  - 100MB만 먼저 로드됨.
  - 나머지 900MB는 필요할 때만 점진적으로 로드됨.


Virtual Memory Larger Than Physical Memory
------

<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-1.png" width="600"></p>

&ensp;구성 요소 설명:<br/>
1. Virtual Memory
* page 0 ~ page v까지 존재하는 가상 메모리의 페이지들이다.
* 각 프로세스는 자신만의 가상 주소 공간을 갖고 있으며, 마치 연속적인 메모리를 쓰는 것처럼 프로그래밍할 수 있다.

2. Memory Map
* 페이지 테이블 또는 매핑 테이블을 의미한다.
* 가상 페이지를 실제 물리 메모리의 어떤 프레임에 매핑할지를 기록해둔 정보이다.
* 예를 들어, page 2 → 물리 메모리의 두 번째 칸으로 매핑됨을 의미한다.

3. Physical Memory
* 실제 컴퓨터의 RAM을 의미한다.
* 크기가 제한되어 있어 모든 가상 페이지를 동시에 담을 수는 없다.
* 자주 쓰는 페이지들만 올라와 있으며, 나머지는 디스크에 보관된다.

4. 디스크/보조 저장소
* 스왑 영역(swap area) 혹은 **백킹 스토어(backing store)**이다.
* 물리 메모리에 없는 가상 페이지들은 여기 저장되어 있다가 필요한 시점에 다시 메모리로 로드된다.

&ensp;동작흐름<br/>
1. 프로세스가 어떤 가상 주소에 접근함.
2. **Memory Map(페이지 테이블)**을 참조하여 해당 주소가 물리 메모리에 존재하는지 확인.
3. 존재하면 바로 접근.
4. 없으면 디스크에서 해당 페이지를 읽어와 물리 메모리에 올림 → 이를 Page Fault라 함.
5. 물리 메모리가 가득 차면 오래된 페이지를 디스크로 내보내고 새로운 페이지를 로드함 (페이지 교체 알고리즘 사용).

&ensp;예시
* 물리 메모리 크기: 4GB
* 가상 메모리 크기: 16GB
* -> 실제 4GB RAM으로 여러 개의 2~3GB급 프로그램을 동시에 실행 가능함.


Virtual-address Space
------

<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-4.png" width="600"></p>

&ensp;가상 주소 공간은 프로세스가 자신만의 주소 공간을 갖는 구조로 물리 메모리와는 독립적이다.<br/>

&ensp;📐구성 및 동작 구조<br/>

* 0번 주소부터 위로:
1. code: 프로그램의 실행 코드(텍스트 영역)
2. data: 전역 변수와 static 변수.
3. heap: 동적 메모리 영역 (malloc, new 등) -> 위쪽 방향으로 증가(grows up)

* Max 주소부터 아래로:
4. stack: 함수 호출 시 지역 변수와 리턴 주소 저장. -> 아래쪽 방향으로 감소 (grows down).

&ensp;📌중간의 비어 있는 부분 (hole)<br/>
* Heap과 Stack 사이에는 비어 있는 공간이 존재
* 이는 각 영역이 동적으로 확장될 수 있도록 여유를 둔 것이다.
* 이 비어 있는 공간은 프로그램 실행 중 점점 채워질 수 있다.

&ensp;🔧 기술적 이점<br/>
1. 최대 주소 공간 활용 (Maximizes address space use)
* 서로 반대 방향으로 커지기 때문에, 전체 주소 공간을 효율적으로 활용할 수 있다.

2. Sparse Address Space (희소한 주소 공간)
* 주소 공간 중 일부를 사용하지 않음으로써 "구멍(hole)"이 생기는데, 이 덕분에 동적으로 링크된 라이브러리 등을 중간에 삽입 가능하게 만든다.

3. System Libraries 공유
* OS는 system library를 여러 프로세스가 공유된 가상 주소에 매핑함으로써 메모리 절약을 실현한다.

4. Shared Memory
* 서로 다른 프로세스 간 데이터를 공유할 때, 특정 페이지를 서로 다른 가상 주소 공간에 읽기/쓰기 가능하게 매핑할 수 있다.

5. fork() 최적화
* 자식 프로세스를 생성할 때, 가상 주소 공간을 복사하는 대신 공유(예: copy-on-write)하여 빠르게 생성 가능하게 하다.

Shard Library Using Virual Memory
------

<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-2.png" width="600"></p>

&ensp;💡프로세스란?
* 컴퓨터에서 프로그램이 실행되면, 프로세스라는 단위로 메모리 공간이 만들어진다. 
* 이 프로세스는 코드, 데이터, 힙(Heap), 스택(Stack) 공간을 따로 가진다.

&ensp;💡라이브러리란?
* 자주 쓰는 기능(예: printf(), scanf())을 모아둔 코드 파일이다.
* 여러 프로그램에서 이 코드를 매번 복사해서 쓸 필요 없이, 한 번만 메모리에 올려서 여러 프로세스가 공유하면 효율적이다.

&ensp;왼쪽과 오른쪽에 있는 건 두 개의 다른 프로세스이다.
* 각 프로세스는 아래와 같이 생긴 자기만의 메모리 공간을 가지고 있다.
  - code (내 코드)
  - data (내 변수들)
  - heap (내가 동적으로 만드는 데이터)
  - stack (함수 호출 시 쌓이는 공간)

&ensp;🔁‘shared library’는?<br/>
* shared library는 여러 프로세스가 같은 코드를 공유해서 쓰는 라이브러리
* 이 부분은 중간에 있는 파란색 shared pages라는 공간을 통해 같은 메모리를 나눠서 사용

&ensp;🔄 왜 이렇게 하나요?<br/>
* ✅ 메모리 절약
  - 예를 들어 100개의 프로그램이 printf()를 쓴다고 가정할게요.
  - 매번 그 코드를 복사해서 100번 넣으면 메모리 낭비가 심해요.
  - 한 번만 메모리에 올려두고, 모든 프로세스가 그걸 함께 사용하면 메모리를 아낄 수 있어요!
* ✅ 속도 향상
   - 불필요한 코드 중복이 없어서 프로그램을 더 빠르게 시작할 수 있다.

* 왼쪽과 오른쪽 프로세스는 각각의 stack, heap, data, code를 가지고 있다.
* 하지만 shared library 부분은 **서로 연결돼서 같은 페이지(shared pages)**를 사용 중이다. 
  - 마치 둘이 공동 책상에 있는 사전 하나를 같이 보는 것과 같다.

&ensp;📌 정리
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-3.png" width="600"></p>


2\. 요구 페이징(Demand Paging)
======
&ensp;📦 Demand Paging (요구 페이징)이란?
* 개념 요약: 필요할 때만 메모리에 페이지를 불러오는 방식
* 쉽게 말하면?: 컴퓨터에서 프로그램 전체를 한 번에 메모리에 올리지 않고, 진짜 필요한 부분만 그때그때 메모리에 불러오는 방식

&ensp;🖼 그림 설명<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-7.png" width="600"></p>

* 왼쪽 회색 박스 = 메인 메모리 (RAM)
  - 프로그램 A와 B가 메모리에 올라가 있는 상태야.
* 오른쪽 파란 박스 = 디스크 (하드디스크나 SSD처럼 느린 저장공간)
  - 프로그램의 나머지 부분이 저장되어 있다.
* 어떤 페이지(예: 프로그램 A의 3번 페이지)가 필요 없어지면 → 디스크로 "swap out" (내려보내기)
* 나중에 프로그램 B가 어떤 페이지를 필요로 하면 → 디스크에서 "swap in" (불러오기)

<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-5.png" width="600"></p>

&ensp;📌 작동 방식 (간단한 흐름)
1. 프로그램이 실행되다가 어떤 페이지가 필요해졌어!
2. 근데 그 페이지가 아직 메모리에 없어!
3. 👉 그래서 페이지 폴트(Page Fault) 발생
4. 운영체제가 디스크에서 필요한 페이지만 메모리로 가져와 (swap in)
5. 그리고 프로그램은 계속 실행해

&ensp;💬 용어 정리
* 페이지(Page): 프로그램을 작게 나눈 조각 (보통 4KB 등)
* 스왑(Swap): 메모리 ↔ 디스크 간의 이동
* 페이지 폴트: 필요한 페이지가 메모리에 없을 때 발생하는 이벤트
* Lazy Swapper: 진짜 필요할 때만 페이지를 가져오는 방식
* Pager: 이 작업을 실제로 수행하는 운영체제 모듈

&ensp;📍 비유로 이해해보기
* 메모리는 냉장고처럼 빠르게 꺼내 쓸 수 있는 공간
* 디스크는 창고처럼 느리지만 큰 공간
* 냉장고에 필요한 음식만 꺼내 두고, 안 쓰는 건 창고에!
* 배고플 때만 음식(페이지)을 꺼내오는 게 Demand Paging이야 🍱

&ensp;✨ 요약
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-6.png" width="600"></p>

basic concepts
------
&ensp;Demand Paging: 컴퓨터가 프로그램 전체를 한 번에 메모리에 올리지 않고, 실제로 필요한 페이지만 메모리에 올리는 방식이다. 즉, "필요할 때만 불러오기!" 전략이다.<br/>
&ensp;💡 왜 사용해요?<br/>
* 📉 메모리 사용량을 줄일 수 있다.
* ⚡ **응답 시간(Response time)**이 더 빨라질 수 있다.
* 👥 동시에 더 많은 프로그램을 실행할 수 있다.

&ensp;🧾 예시 흐름(Demand Paging 동작 과정)<br/>
1. 어떤 프로그램이 실행된다다.
2. 실행 중인 부분에 필요한 **페이지(Page)**가 메모리에 없다면?
3. 운영체제는 디스크에서 해당 페이지를 불러와서 메모리에 올린다.
4. 이 과정을 **페이지 폴트(Page Fault)**라고 한다.

&ensp;🧊 Lazy Swapper & Pager<br/>
* Lazy Swapper (게으른 스와퍼): 정말 필요한 페이지만 가져왔다.
* Pager (페이지 관리자): 어떤 페이지가 필요한지를 판단해서 메모리에 올리는 역할.

Valid-Invalid Bit(유효/무효 비트)
------
&ensp;📌 개념
* 페이지 테이블의 각 항목에는 valid-invalid bit가 있어요.
* 이 비트는 페이지가 메모리에 올라와 있는지 아닌지를 알려줘요.
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-8.png" width="600"></p>

&ensp;🛠 동작
1. CPU가 어떤 페이지를 참조하려고 할 때,
2. MMU가 페이지 테이블을 확인하고,
3. 비트가 v이면 정상 접근 가능!
4. 비트가 i이면 → Page Fault 발생 → 디스크에서 메모리에 불러옴!

&ensp;🔄 요약
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-9.png" width="600"></p>


Page Table When Some Pages Are Not in Main Memory
------

<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-47.png" width="600"></p>

&ensp;이 그림은 가상 메모리(Virtual Memory)시스템에서 일부 페이지만 실제 물리 메모리에 올라와 있는 상황을 보여준다. 가상 메모리는 프로세스마다 독립적인 메모리 공간을 가지도록 해주는 기술이다.<br/>

&ensp;🔹 구성 요소 설명<br/>
1. Logical Memory
* 이건 프로세스가 인식하는 메모리(프로세스의 입장에서 주소 0~7)
* A~ H까지 총 8개의 페이지가 논리 메모리 상에 존재한다.

2. Page Table
* 각 논리 페이지가 물리 메모리 상 어디에 있는지를 알려주는 표
* frame 열: 해당 논리 페이지가 올라간 물리 프레임 번호
* valid-invalid bit 열: 
  - v: valid -> 지금 메인 메모리에 있음
  - i: invalid -> 지금 메인 메모리에 없음(디스크에 있음)
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-9.png" width="600"></p>


3. Physical Memory
* 실제 컴퓨터 하드웨어 상의 메모리 공간
* 지금은 A, C, F 세 페이지만 메모리에 올라와 있음

4. 디스크
* 디스크는 **메모리에 올라오지 않은 나머지 페이지들(B, D, E, G, H)**을 저장하고 있다.
* 필요하면 여기서 불러와요 (-> demand paging)

&ensp;🔄 동작 시나리오 예시<br/>
&ensp;📌 CPU가 페이지 D에 접근하려고 한다면?
1. Page Table을 확인
2. D는 invalid(i) 상태니까 -> Page Fault 발생
3. 운영체제가 디스크에서 D를 찾아 메모리로 불러옴(swap-in)
4. Page Table을 업데이트: D -> 프레임 번호 설정 + valid 비트로 변경

&ensp;💡 핵심 포인트<br/>
* 모든 페이지가 한 번에 메모리에 올라올 필요는 없다
* 필요한 페이지만 가져오는 Demand Paging 방식은 메모리를 효율적으로 사용하게 해준다
* Page Table + Valid Bit + Disk 조합으로 가상 메모리를 구현할 수 있다.

&ensp;🔧 요약<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-11.png" width="600"></p>

Page Fault
------
&ensp;page fault는 CPU가 어떤 데이터를 사용하려고 했는데 그 데이터(또는 코드)가 지금 메인 메모리에 없을 때 발생하는 오류<br/>

&ensp;필요한 데이터가 메모리에 없어서 당황한 CPU가 운영체제에게 도움을 요청하는 상황이라고 생각하면 된다.<br/>

&ensp;Page Fault 발생 시 처리 단계 <br/>
1. 운영체제가 확인
&ensp;운영체제(OS)은 페이지 테이블을 확인해서 왜 접근이 실패했는지 확인<br/>
* Invalid reference(잘못된 접근): 그 주소 자체가 틀림 -> 프로그램 강제 종료(abort)
* Just not in memory(단지 메모리에 없을 뿐): 디스크에 있음 -> 아래 단계를 계속 진행

2. 빈 메모리 프레임 찾기
* 빈 공간이 있는지 확인
* 없다면 기존 페이지 하나를 내보내고(swap out)공간을 만듦

3. 디스크에서 메모리로 데이터 가져오기(swap-in)
* 디스크에 있던 해당 페이지를 메모리로 불러와요
* 보통 몇 ms 정도 걸리는 무거운 작업이다.

4. 페이지 테이블 갱신
* 페이지 테이블의 해당 페이지 entry를 valid 상태를 설정(v)
* 그리고 해당 페이지가 들어온 **물리 주소(프레임 번호)**도 업데이트한다.

5. 중단된 명령 재실행
* 페이지를 찾지 못해 멈췄던 명령을 처음부터 다시 실행시킨다.
* 이번에 메모리에 있으니까 정상적으로 실행된다.

&ensp;요약: Page Fault 처리 흐름
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-12.png" width="600"></p>

&ensp;💡 비유로 이해하면?
&ensp;📚 학생이 참고서를 보려고 했는데 가방에 없어서 사물함에서 꺼내오는 과정과 비슷해요!<br/>
1. 책 찾음(CPU)
2. 없어서 혼남(Page fault)
3. 선생님한테 말함(OS)
4. 사물함에서 가져옴(disk -> memory)
5. 책을 책상에 올리고 계속 공부함(재실행)

steps in handling a page fault(페이지 폴트 처리 단계)
-------

<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-46.png" width="600"></p>

&ensp;💥 상황 설명<br/>
&ensp;CPU가 어떤 데이터 M을 사용하려고 할 때 그 데이터가 현재 메인 메모리에 없다면 page fault가 발생한다.<br/>

&ensp;단계별 설명<br/>

1. reference M (참조 시도)
* 사용 중인 프로그램이 주소 M 접근하려고 한다.
* CPU는 MMU(메모리 관리 장치)를 통해 페이지 테이블을 확인한다.
&ensp;-> 이때 페이지 테이블의 valid-invalid 비트를 봤더니 i(invalid) 상태인 것이다.<br/>
&ensp;즉 현재 메모리에 M이 없음<br/>

2. trap(예외 발생, OS 발생)
* MMU가 페잊 폴트를 탐지하면 trap이라는 신호를 발생시켜서 **운영체제(OS**)**에게 제어권을 넘긴다.
&ensp;-> 이 데이터 없어요!도와주세요! 라는 상황

3. Page is on backing store(디스크 확인)
* 운영체제는 해당 페이지가 디스크(Backing Store)에 저장되어 있는지 확인한다.
* 디스크에 있다면 -> 메모리로 옮겨야겠지

4. bring in missing page (디스크에서 가져오기)
* 디스크에 있던 그 페이지를 메모리로 읽어옵니다 (swap-in 작업).
* 이 작업은 수 밀리초까지 걸릴 수 있는 무거운 I/O 작업이다.

5. reset page table (페이지 테이블 갱신)
* 해당 페이지가 올라온 물리 프레임 번호를 기록하고 valid-invalid 비트를 'v'(valid) 로 바꾼다.
&ensp;“이제는 이 페이지 메모리에 있어요~” 라고 표시해주는 거다.

6. restart instruction (명령 재실행)
* 처음에 문제가 발생했던 load M 명령을 다시 실행한다.
* 이번엔 페이지가 메모리에 있으므로 정상 실행된다.

&ensp;🔁 요약: 그림 흐름 순서<br/>
1. 프로그램이 메모리에 없는 페이지 M을 요청
2. MMU → trap → OS 호출
3. OS가 디스크에서 M을 찾음
4. 디스크에서 M을 메모리로 복사
5. 페이지 테이블 갱신 (valid 표시)
6. 처음 명령 다시 실행 (이번엔 성공!)

&ensp;💡 쉽게 예를 들면?<br/>
&ensp;📚 "책상에서 책을 찾으려다 없어서 책장을 뒤져서 다시 가져오는 과정"<br/>
* 책이 책상에 없으면 (page fault)
* 책장(디스크)에서 찾아서 (backing store)
* 책상(메모리)에 올려놓고 (swap-in)
* 다시 공부 시작 (명령 재실행)

Aspects of Demand Paging
------

&ensp;✅ Extreme case: 시작할 때 아무 페이지도 메모리에 없다.<br/>
* 프로세스가 실행을 시작할 때, 메모리에 아무 페이지도 없는 상태로 시작할 수 있다.
* 운영체제(OS)는 먼저 프로그램의 첫 번째 명령어로 점프하려고 하지만 그 명령어가 들어 있는 페이지가 메모리에 없으면 → page fault 발생!
&ensp;그리고 나서 다른 코드들도 실행하려고 할 때마다 페이지 폴트가 생긴다.<br/>

&ensp;🔵 Pure demand paging (순수 요구 페이징)<br/>
* 시작할 때 메모리에 아무것도 없는 상태에서 필요한 페이지가 요청될 때마다 하나씩 가져오는 방식이다.
* 장점은 메모리 절약, 단점은 실행 초기 느릴 수 있음.

&ensp;🔄 한 명령이 여러 페이지에 접근할 수도 있다.<br/>
&ensp;예: x = A[i] + B[j] 같은 명령은...<br/>
* A[i] -> 첫 번째 페이지
* B[j] -> 두 번째 페이지
* x -> 세 번째 페이지
&ensp;-> 한 줄에 세 개의 페이지 폴트가 날 수 있다.<br/>

&ensp;🧠 Locality of Reference (참조 지역성)<br/>
&ensp;현실에서는 대부분의 프로그램이 같은 코드나 데이터를 반복적으로 사용한다.<br/>
&ensp;예: for문, 함수 호출 등
  - 그래서 보통 한 번 페이지를 불러오면 그 페이지를 오랫동안 계속 사용해야 한다.
  - 덕분에 실제로는 페이지 폴트가 자주 일어나지 않는다.

&ensp;🛠 Demand Paging에 필요한 하드웨어 지원
1. Page Table + Valid/Invalid Bit
* 각 페이지가 메모리에 있는지(v), 아니면 **없는지(i)**를 표시한다.
* MMU가 주소를 변환할 때 이 비트를 확인해서 페이지 폴트 발생 여부 판단.

2. Secondary Memory (Swap Space)
* 페이지가 메모리에 없을 경우 → 디스크에 저장된 페이지를 찾아서 메모리에 올려야 한다. 
* 이걸 위해 디스크에 페이지 저장 공간인 swap 공간이 필요합니다.

3. Instruction Restart
* 페이지 폴트 때문에 실패했던 명령어는, 페이지가 메모리에 로드된 후 다시 실행되어야 해요.
* 이를 위해 CPU는 그 명령어를 기억하고 있다가 다시 실행한다.

Performance of Demand Paging
------

&ensp;Demand Paging 동작 방식 (기본 원리)

1. 프로세스 실행 시작 시, 메모리에 아무 페이지도 없음
2. CPU가 어떤 메모리 주소에 접근하면 → Page Fault 발생
3. 운영체제(OS)가 디스크에서 해당 페이지를 메모리에 불러옴
4. 페이지 테이블에 해당 페이지가 올라온 것 표시 (valid bit = v)
5. 원래 하려던 명령 재시작

&ensp;⏱️ EAT (Effective Access Time) 계산<br/>
&ensp;✅ 기본 공식<br/>
&ensp;EAT = (1 - p) × 메모리 접근 시간  + p × (페이지 폴트 처리 시간)<br/>
* p : 페이지 폴트 확률
* 페이지 폴트 처리 시간 = 인터럽트 처리 + 디스크 입출력 + 재시작

&ensp;✅ 예시<br/>
* 메모리 접근 시간: 200ns
* 페이지 폴트 처리 시간: 8ms = 8,000,000ns
* 1/1000의 확률로 페이지 폴트 발생한다고 가정

&ensp;EAT = 0.999 × 200 + 0.001 × 8,000,000  ≈ 8,200ns (느려짐) <br/>
&ensp; 무려 40배 느려짐 → 성능 저하 크므로 p는 매우 작아야 함<br/>

&ensp;⚙️ 최적화 방법<br/>
1. Locality of Reference 활용
* 실제 프로그램은 같은 코드, 같은 데이터에 반복 접근하는 경향 있음
* 덕분에 일부 페이지만 올려도 동작 가능
2. Swap Space 활용
* 디스크에서 Swap 전용 공간을 만들어 페이징 속도 개선
3. Fork() 최적화
* 자식 프로세스 생성 시 페이지 복사 대신 공유 방식 사용
* Copy-on-write 방식으로 메모리 절약

3\. Copy-on-Write
======
&ensp;🧠 Copy-on-Write란?<br/>
&ensp;Copy-on-Write는 부모 프로세스와 자식 프로세스가 처음에는 메모리를 공유하다가, 누군가 수정하려고 할 때 비로소 **진짜 복사(copy)**를 만드는 기술이다.<br/>

&ensp;📌 언제 쓰이냐면?<br/>
&ensp;fork() 시스템 콜을 사용할 때!<br/>
  - 부모 프로세스를 복사해서 자식 프로세스를 만들면, 부모의 메모리 전체를 그대로 복사하는 건 비효율적이다.
  - 왜냐면... 자식 프로세스가 아무것도 안 바꾸면 그 복사본은 쓸모가 없다!!
&ensp;그래서 등장한 게 Copy-on-Write.

&ensp;🔄 어떻게 동작할까?<br/>
1. 부모와 자식 프로세스는 처음엔 같은 메모리 페이지를 공유함.
2. 누군가 그 페이지를 수정하려고 하면
3. 그제야 복사본을 만들어서 수정은 복사본에서 일어남.
4. 덕분에 불필요한 복사 최소화, 메모리 절약, 속도 빠름

&ensp;💡 더 알아두면 좋은 것들<br/>
* vfork()는 fork()보다 가벼운 변형.
  - 자식이 exec() 호출할 것만 기대하고 사용됨.
  - 부모는 자식이 exec여전히 COW 방식 주소 공간 공유.
  - 여전히 COW 방식 주소 공간 공유.

&ensp;🧊 zero-fill-on-demand?<br/>
* "빈 페이지"를 미리 만들어 두는 메모리 풀에서 가져옴
* 페이지가 처음 요청될 때 0으로 채워서 제공

* Before Process 1 Modifies Page C
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-13.png" width="600"></p>

* process1와 process2는 둘 다 page A, page B, page C를 같은 물리 메모리를 가리키고 있다.
* 이 단계에서는 아무도 수정을 하지 않았기 때문에 그냥 공유하고 있다.
* 메모리 낭비 없이 효율적이다.

* After Process 1 Modifies Page C
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-14.png" width="600"></p>

* 이제 process1이 page C를 수정하려고 한다.
* 그러자 운영체제가 page C를 복사해서 새로운 물리 메모리 공간에 만들어 준다.
* process1는 새로 복사된 페이지를 사용하고 process2는 원래 page C를 그래도 사용한다.
* 각자 따로 쓰는 구조가 된다.

&ensp;장점<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-15.png" width="600"></p>

4\. 페이지 교체(Page Replacement)
======

&ensp;🧠 문제 상황: "Free Frame이 없다면?"<br/>
&ensp;운영체제는 프로그램을 실행하기 위해 메모리(Frame)를 사용한다. 그런데 모든 메모리가 이미 어떤 페이지로 가득 찬 상황에서, 새로운 페이지를 불러와야 한다면 어떻게 할까? -> 이때 필요한 것이 페이지 교체(Page Replacement) 이다.<br/>

&ensp;🔄 Page Replacement란?
&ensp;**"현재 메모리에 있는 페이지 중 안 쓰는 걸  찾아서, 내보내고 새로운 걸 대신 넣는 것"**이다.<br/>
&ensp;즉, 메모리가 꽉 찼을 때,<br/>
* 지금 당장 안 쓰는 페이지를 찾아서, 디스크(보조기억장치)로 스왑 아웃(swap out) 하고, 필요한 새로운 페이지를 스왑 인(swap in) 하는 거다.

&ensp;🔍 페이지 교체가 필요한 실제 예시<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-16.png" width="600"></p>
&ensp;상황 설명: <br/>
* user1과 user2가 각각 4개의 페이지를 가지고 있다.
* 물리 메모리에는 총 8개의 프레임만 존재
* 현재 모든 메모리 프레임은 꽉 차 있음

&ensp;예를 들어<br/>
* user1이 load M이라는 명령을 실행해서 M페이지를 접근하려고 하는데 M페이지는 현재 메모리에 없음 -> 즉 Page Fault 실행
* 그런데 모든 메모리가 가득 차 있어서 바로 넣을 수 없다.

&ensp;📌 그럼 어떻게 해야 할까?<br/>
&ensp;-> 어떤 페이지를 골라서 쫓아내야 힌다.(예: User2의 B페이지가 오래동안 사용되지 않았다면 이것을 디스크에 내보냄)<br/>

&ensp;교체 방식 예시 (Page Replacement Algorithms)<br/>
&ensp;운영체제는 어떤 페이지를 교체할지 알고리즘을 통해 결정한다.<br/>
&ensp;대표 알고리즘:<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-17.png" width="600"></p>

&ensp;💡 핵심 정리<br/>
* 페이지 교체는 메모리가 꽉 찼을 때 발생
* 안 쓰는 페이지를 내보내고 필요한 페이지를 불러옴
* 이를 잘하기 위해 OS는 Page Replacement Algorithm을 사용
* 성능을 위해 교체 횟수를 줄이는 것이 중요

Basic Page Replacement
-----
&ensp;컴퓨터는 프로그램을 실행할 때 프로그램의 일부 내용을 **주 기억장치(RAM)**에 올려야 실행할 수 있다. 이때 사용하는 단위가 Page이다. 하지만 RAM은 용량이 제한되어 있어서 모든 페이지를 다 올릴 수 없다. 그래서 어떤 페이지가 필요할 때 RAM에 공간이 없으면 기존 페이지 중 하나를 뺀 다음 새 페이지를 넣는 작업이 필요하다. 이걸 **Page Replacement**라고 한다.

&ensp;🧊 상황 예시: 프레임이 꽉 찼을 때<br/>
* 사용자 1: A, B, C, D, E, F, G, H 페이지가 필요함
* 하지만 RAM은 3개밖에 못 담음(ex. A, B, C)
&ensp;만약 이 상태에서 G페이지가 필요하다면 -> 기존의 어떤 페이지를 하나를 내보내고 G를 넣어야 한다.

&ensp;기본 절차
1. 디스크에서 읽어올 페이지 찾기
* 예: "G" 페이지가 필요하다!
2. RAM에서 빈 공간 있는지 확인
* 빈 공간이 있다면 바로 넣기
* 빈 공간이 없다면, 기존 페이지 중 하나를 골라서 내보냄 (victim page)
3. victim 페이지가 수정되었는지 확인
* 수정됐다면 디스크에 저장 (→ dirty 페이지라 부름)
* 수정 안 됐다면 그냥 버려도 됨
4. 새 페이지(G)를 디스크에서 읽어와서
* RAM의 빈 공간에 넣기
* Page Table 정보도 함께 수정
5. 원래 하려던 명령어 재시작

<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-18.png" width="600"></p>

1. 현재 frame f에 있는 페이지를 디스크로 다시 저장함 (swap out)
2. 그 페이지의 valid bit를 i로 바꿈 → 더 이상 메모리에 없음 표시
3. 디스크에서 새로운 페이지를 읽어 RAM에 올림 (swap in)
4. Page Table의 정보도 새 페이지 기준으로 수정함

&ensp;🧩 victim page란?<br/>
&ensp;**"RAM에서 내보낼 페이지"**를 의미한다.RAM에 공간이 부족할 때 어떤 페이지를 선택해서 내보낼지를 정해야 한다. 이때 쓰는 알고리즘이 있다.
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-19.png" width="600"></p>

Page Replacement Algorithms(페이지 교체 알고리즘)
------

&ensp;🔍이건 뭐 하는 알고리즘인가요?<br/>
* 메모리에 빈 공간(프레임)이 없을 때 어떤 페이지를 내보낼지 결정하는 알고리즘이다.

&ensp;💡 왜 중요하죠?<br/>
* 페이지를 잘못 골라서 자주 다시 필요하게 되면 **계속 페이지 폴트(Page fault)**가 발생하고 프로그램 속도가 심각하게 느려질 수 있다.

&ensp;🎯 목표<br/>
* 가장 적은 Page Fault를 발생시키는 게 이 알고리즘의 목표이다.

&ensp;예를 들어 볼게요 👇<br/>
1. 메모리에 A, B, C 페이지가 있음
2. 새로운 D 페이지를 넣어야 하는데 공간이 없음
3. A, B, C 중에 하나를 제거해야 한다.
4. 이때 무엇을 기준으로 하나를 선택할까? -> 이걸 정하는 게 Page Replacement Algorithm

&ensp;📌 대표적인 페이지 교체 알고리즘들<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-20.png" width="600"></p>


Frame-allocation algorithm(프레임 할당 알고리즘)
------

&ensp;🔍 이건 뭐 하는 알고리즘인가요?<br/>
* 각 프로세스에게 메모리 프레임을 몇 개 줄지 결정하는 것이다.
* 다시 말해 여러 프로세스가 동시에 실행될 때:
  - A 프로세스에게 프레임 몇 개
  - B 프로세스에게 프레임 몇 개?

&ensp;🎯 목표<br/>
* 공평하게 나누되 성능도 잘 나오는 방식으로 분배해야 해요.

&ensp;💡 예를 들어 설명해볼게요<br/>
* RAM 전체 프레임: 12개
* 현재 실행 중인 프로세스: 3개
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-21.png" width="600"></p>

&ensp;이렇게 할당을 어떻게 하느냐에 따라 전체 시스템 성능에 큰 영향을 줘요<br/>

&ensp;🎓 쉽게 요약하면<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-22.png" width="600"></p>

&ensp;💬 정리<br/>
* 페이지 교체는 RAM이 부족할 때 어떤 데이터를 버릴지를 정하는 거고,
* 프레임 할당은 각 프로세스에게 메모리 공간을 얼마나 줄지를 정하는 거다.
&ensp;둘 다 잘 설계하지 않으면
  - 프로세스가 느려지고
  - 자꾸 페이지 폴트가 나고
  - 컴퓨터가 뻑뻑해지는 원인이 된다.


First-In-First-Out (FIFO) Algorithm
------

&ensp;가장 먼저 메모리에 들어온 페이지를 가장 먼저 교체하는 방식이다. 즉 오래된 순서대로 메모리에서 제거한다.<br/>

&ensp;🔍 예시 설명<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-23.png" width="600"></p>

&ensp;Reference String: 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1<br/>
&ensp;-> 이건 CPU가 차례대로 접근하는 페이지 번호를 나타내는 문자열이다.<br/>
&ensp;가정: <br/>
* 사용할 수 있는 프레임 수는 3개이다.
* 즉 한 번에 메모리에 3개의 페이지만 넣을 수 있다.

&ensp;⛳ 동작 과정<br/>
* 처음엔 메모리가 비어있음 -> 페이지를 넣기만 하면 돼
* 4번째 페이지 2를 넣을 때 이미 3개가 찼기 때문에 가장 오래된 페이지 7을 제거해야 해
* 이런 방식으로 계속 가장 오래된 페이지를 제거하고 새로운 페이지를 넣음

&ensp;📉 Belady's Anomaly란? <br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-24.png" width="600"></p>

&ensp;**Belady의 모순(Belady’s Anomaly)**은 다음과 같은 이상한 현상을 말해:<br/>
&ensp;프레임 수를 늘렸는데도 페이지 폴트가 더 많이 발생하는 현상<br/>
&ensp;일반적으로 프레임이 많으면 더 좋은 성능이 나올 것 같다? -> FIFO 방식에선 그렇지 않은 경우가 있다.<br/>

&ensp;✅ Optimal Page Replacement Algorithm
------

&ensp;앞으로 가장 오랫동안 사용되지 않을 페이지를 교체하는 방식이다.<br/>
&ensp;예를 들어:<br/>
* 메모리에 있는 페이지 중에서,
* 앞으로 한참 뒤에 쓰이거나 다시 안 쓰일 페이지를 골라서 빼는 거다.

&ensp;❗ 왜 "Optimal"일까?<br/>
* 이 알고리즘은 **가장 적은 수의 페이지 폴트(page fault)**를 발생시키는 이론적인 최선의 방법이다.<br/>
* 단점? 현실에서 이 알고리즘을 쓸 수 없다.<br/>
&ensp;-> 왜냐하면 앞으로 어떤 페이지가 언제 사용될지 우리가 실제로는 알 수 없기 때문이다.<br/>
&ensp;그래서 이 알고리즘은 주로 다른 알고리즘(LRU, FIFO 등)의 성능을 비교 평가할 때 사용된다.<br/>

&ensp;📌 예시 설명<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-25.png" width="600"></p>

&ensp;이건 CPU가 차례대로 접근하려는 페이지들 리스트이다.<br/>
&ensp;프레임 수: 3개<br/>
&ensp;-> 한 번에 메모리에 3개 페이지만 넣을 수 있다. <br/>

&ensp;⛳ Optimal 알고리즘 동작 방식<br/>
&ensp;예를 들어 설명해보면:
1. 처음에 7, 0, 1이 차례로 들어와서 3개 프레임 다 채워짐(페이지 폴트 3번 발생)
2. 그 다음 2가 들어오려고 한다. -> 이미 프레임이 꽉 찼으니 하나를 빼야 한다.
  &ensp;이때 Optimal은 앞으로 가장 늦게 사용될 페이지를 뺀다.<br/>
  - 앞으로 7은 19번째에 등장함
  - 0은 4번째 뒤
  - 1은 13번째 뒤
  &ensp;-> 그래서 7을 제거하고 2를 넣음<br/>
3. 이런 식으로 계속해서 가장 나중에 쓰일 페이지를 교체하면서 진행된다.

&ensp;장단점 정리<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-26.png" width="600"></p>

&ensp;💡 현실에서는?<br/>
&ensp;우리는 이 알고리즘을 직접 사용하지는 않지만, -> LUR(Least Recently Used)같은 알고리즘이 Optimal을 흉내내기 위한 현실적인 대안이다.<br/>

Least Recently Used (LRU) Algorithm
------

&ensp;**LRU(Least Recently Used)**는 가장 오랫동안 사용되지 않은 페이지를 교체하는 페이지 교체 알고리즘이다.<br/>
&ensp;📌 핵심 아이디어<br/>
* 지금까지의 과거 사용 기록을 바탕으로
* 최근에 사용된 페이지는 앞으로도 사용할 가능성이 높고 오랫동아 안 쓴 페이지는 당분간도 안 쓸 가능성이 크다!라는 가정을 이용

&ensp;💡 어떻게 작동해?<br/>

<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-27.png" width="600"></p>

&ensp;🔁 LRU 동작 원리 (간단히 시뮬레이션)<br/>
1. 7, 0, 1 -> 페이지 폴트 발생하며 채워짐(빈 프레임 3개)
2. 다음에 2가 오면 교체 필요
  - 지금 프레임에 있는 건 -> 7 0 1
  - 최근 사용된 시간 순서: 1(가장 최근) 0 7(가장 오래)
  - -> 7이 가장 오래 전에 사용됨 -> 7을 제거하고 2를 넣음
3. 그 다음도 같은 방식으로 계속 진행

&ensp;📊 페이지 폴트 수<br/>
* FIFO: 15 faults
* LRU: 12 faults
* Optimal: 9 faults
&ensp;-> LRU는 현실적으로 자주 사용되는 알고리즘<br/>
&ensp;-> FIFO보다 효율 좋고, Optimal보다는 못하지만 구현 가능<br/>

&ensp;🛠️ 어떻게 구현해?<br/>
&ensp;LRU는 가장 최근 사용 시점을 계속 추적해야 한다.<br/>
&ensp;구현 방법 예시:<br/>
1. 시간 기록법 (Timestamp)
  - 각 페이지마다 마지막으로 접근한 시간 기록
  - 교체할 때 가장 오래된 시간 가진 페이지 선택
2. 스택/리스트 이용
  - 페이지가 참조되면 그걸 스택의 맨 위로 올린다.
  - 스택은 항상 최근에 사용된 순서대로 정렬돼 있다.
  - 가장 오래된 건 스택 맨 아래에 있어서 바로 꺼내버릴 수 있다.
  - 📌 단점: 참조할 때마다 스택을 수정해야 하므로 비용이 큼.
3. 하드웨어 지원
  - 각 페이지마다 카운터 값을 하나 둔다.
  - 페이지가 참조될 때마다 현재 시각(시스템 시계 값)을 카운터에 기록
  - 페이지를 교체할 때는 카운터 값을 보면서 가장 오래된 시간 값을 가진 페이지를 선택
  - 📌 단점: 테이블 전체를 훑는(검색) 작업이 필요해 성능이 떨어질 수 있음

&ensp;스택 기반 구현 예시<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-28.png" width="600"></p>

* 상황 설명
* 페이지 참조 순서(reference string): 4 → 7 → 0 → 7 → 1 → 0 → 1 → 2
  - tack Before a : 가장 최근에 사용된 페이지는 2, 가장 오래된 건 4
  - Stack After b : 페이지 7이 또 사용돼서 맨 위로 이동함. 나머지는 순서만 밀렸을 뿐 삭제되지 않음.
&ensp;이 구조 덕분에 "가장 오래전에 쓰인 페이지"가 항상 맨 아래에 있어서
교체 대상 선택이 빠르고 직관적이다.

&ensp;🌟 LRU의 장점<br/>
* FIFO보다 성능이 좋음 (Page Fault 적음)
* Belady’s Anomaly(프레임 늘려도 page fault 늘어나는 이상현상)가 발생하지 않음!

&ensp;⚖️ 비교 요약<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-29.png" width="600"></p>

* 🔔 기억 꿀팁
  - Optimal은 미래를 알고 행동하는 “신” 같은 전략
  - LRU는 과거를 보고 예측하는 “현실적 사람” 같은 전략

LRU Approximation Algorithms
------

&ensp;🧠 왜 근사 알고리즘이 필요한가요?<br/>
&ensp;LRU를 구현하려면:<br/>
* 모든 페이지 접근 시간 기록이 필요
* 최근 사용 시각을 저장하려면 하드웨어나 복잡한 자료구조 필요 -> 느리고 비효율적임.
&ensp;그래서 하드웨어 부담 없이 흉내만 내는 방식이 나왔습니다 -> Approximation Algorithms<br/>

&ensp;💡 Reference Bit 기반 근사 방법<br/>
&ensp;✅ Reference Bit란?<br/>
  - 각 페이지 테이블 항목에 1비트짜리 flag를 둔다
  - 이 비트는 CPU가 페이지를 읽거나 쓰면 1로 바뀐다.
  - OS는 이 reference bit를 확인해서 해당 페이지가 최근에 사용됐는지 파악한다.
&ensp;❗ 중요한 특징:<br/>
* reference bit = 1 -> 최근에 사용됨
* reference bit = 0 -> 오래 사용 안 됨 (교체 후보)

&ensp;1. 🔄 Second-Chance Algorithm (= Clock Algorithm)<br/>
&ensp;Reference Bit를 기반으로 만든 LRU 근사 알고리즘이다.<br/>
&ensp;📘 동작 방식 요약:<br/>
1. 페이지들이 원형 큐(시계 형태)에 저장되어 있다고 가정.
2. 교체할 차례가 된 페이지를 가리키는 포인터 존재 (next victim)
3. 포인터가 가리키는 페이지의 reference bit를 확인:
  - 0이면 교체 (replace)
  - 1이면 비트를 0으로 리셋하고, 포인터를 다음 페이지로 이동
4. 조건을 만족할 때까지 시계방향으로 계속 순회. -> 그래서 Clock Algorithm이라고 부름

<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-30.png" width="600"></p>

&ensp;(a)<br/>
* next victim 포인터가 reference bit가 1인 페이지를 가리킴.
* 그 페이지는 한 번 기회를 더 받음 -> bit를 0으로 바꾸고 포인터 이동

&ensp;(b)<br/>
* 다음 페이지가 reference bit = 0 -> 이 페이지는 교체 대상이 됨

&ensp;✅ 장점 vs 한계<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-31.png" width="600"></p>

&ensp;2. Enhanced Second-Chance Algorithm<br/>

&ensp;📌 기본 아이디어:<br/>
&ensp;일반적인 Second-Chance(Clock) 알고리즘은 페이지에 "참조 비트(Reference Bit)"만을 사용하는데, Enhanced 버전은 참조 비트 + 수정 비트(Modified Bit) 를 함께 사용해서 더 똑똑하게 어떤 페이지를 교체할지를 결정한다.<br/>

&ensp;📋 4가지 경우:<br/>
&ensp;이 두 비트를 쌍으로 보고 다음과 같이 우선순위를 정함:<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-32.png" width="600"></p>

&ensp;📌 장점:<br/>
&ensp;더 "지능적"으로 희생 페이지(victim page)를 골라서 성능 향상 가능<br/>

Counting Algorithms (카운팅 알고리즘)
------

&ensp;이 알고리즘은 **페이지가 몇 번이나 참조됐는지(사용됐는지)**를 세서 교체할지 말지를 결정한다.<br/>
* LFU (Least Frequently Used)
  - 가장 적게 참조된 페이지를 교체
  - 이유: “자주 안 쓰였으니까 빼도 괜찮겠지?”
* MFU (Most Frequently Used)
  - 가장 많이 참조된 페이지를 교체
  - 이유: “이제 막 불러와서 자주 쓸 것처럼 보이는 페이지는 아닐 수도 있음.”
* ⚠️ 단점:
  - 페이지마다 카운트를 관리해야 하니까 구현 복잡도 높고 성능 저하 우려
  - 그래서 실제로는 잘 안 씀

Page-Buffering Algorithms (페이지 버퍼링 알고리즘)
------

&ensp;페이지 교체가 필요한 순간에 바로 victim page를 고르기보다, **"여유 페이지 프레임을 미리 준비해두는 방식"**이다.<br/>
&ensp;📌 기본 아이디어:<br/>
* 페이지 교체는 시간이 오래 걸림 → 그래서 교체할 페이지를 미리 정해두고 따로 저장
* 교체가 필요할 땐 이미 준비된 페이지를 써서 시간 단축
&ensp;📋 세부 전략:<br/>
1. Free Frame Pool 유지: 여유 프레임들을 미리 모아두고, 바로 사용할 수 있게 한다.
2. Victim Page 따로 저장:
  - 여유 있을 때, 디스크로 저장해두고 메모리에서는 제거한다.
  - Modified page는 백업한 후 제거한다.
3. Re-use 최적화<br/>
  - 만약 victim page가 다시 필요하면? 디스크에서 다시 가져오는 대신 미리 저장해둔 것을 복원한다.
  
&ensp;📌 장점:<br/>
  - 잘못된 victim을 선택했을 때도 다시 빨리 복구할 수 있음.
  - 페이지 교체에 따른 패널티 감소 (즉, 느려지는 정도가 줄어듦)


Applications and Page Replacement
------

&ensp;📦 페이지 교체와 어플리케이션<br/>
&ensp;운영체제(OS)는 지금 어떤 페이지를 메모리에 두고 어떤 걸 빼야 할까를 계속 고민한다. 근데 OS는 미래를 알 수 없기 때문에 예측(guess)해야 한다.<br/>

1. 🔍 OS는 미래를 예측해야 한다
&ensp;운영체제가 페이지 교체 알고리즘(FIFO, LRU 등)을 사용하는 이유: <br/>
* 다음에 어떤 페이지가 필요할까를 맞추기 위해서이다.
* 근데 이건 쉽지 않다. 왜냐하면 사용자가 다음에 뭘 할지 알 수 없으니까!

2. 💡 어떤 프로그램은 더 잘 안다
&ensp;예를 들어 데이터베이스(DB)프로그램은:<br/>
* 자기가 어떤 데이터를 자주 사용할지 자기 자신이 잘 안다.
* 그래서 OS보다 더 똑똑하게 메모리를 관리할 수 있다.

3. ⚠️ 더블 버퍼링(Double Buffering) 문제
&ensp;메모리를 많이 쓰는 프로그램은 다음과 같은 문제가 생긴다.<br/>
* OS쪽 : I/O 버퍼를 메모리에 유지함
* 프로그램 쪽 : 자기 작업을 위해 같은 데이터를 또 메모리에 유지함
&ensp;같은 데이터를 두 번이나 메모리에 저장해서 낭비가 생긴다.<br/> 

4. 🚀 Raw Disk Mode (직접 디스크 접근)
&ensp;운영체제가 이렇게 낭비되는 걸 막기 위해:<br/>
* Raw disk mode 라는 방식을 제공
* 이건 운영체제가 메모리에 개입하지 않고 프로그램이 직접 디스크에 접근하도록 허용하는 거다.

&ensp;장점:<br/>
* 중복된 버퍼링(buffering) 없음
* 락(locking) 같은 추가 제어도 우회
* 성능 향상

5\. 프레임의 할당(Allocation of Frames)
=======

&ensp;🧠 프레임(Frame)이란?<br/>
&ensp;운영체제에서 메모리를 사용할 때 물리 메모리는 작은 단위인 프레임(frame)으로 나눠진다. 프로세스가 실행되기 위해서는 이 프레임들이 필요하다.<br/>

1. 각 프로세스는 최소한의 프레임이 필요하다
* 어떤 프로그램(프로세스)이 실행되기 위해서는 최소 몇 개의 프레임이 꼭 필요
* 예를 들어 어떤 명령어가 여러 개의 페이지에 걸쳐 있다면, 그만큼의 프레임이 메모리에 있어야 실행이 가능

&ensp;💡 예시: IBM 370 컴퓨터<br/>
* 어떤 SS MOVE 명령어는 6바이트 크기였고 
* 그 명령어가 두 개의 페이지에 걸쳐 있을 수도 있다.
* 또 데이터를 복사해야 하니까:
  - 2페이지는 어디서 가져올지
  - 2페이지는 어디로 보낼지도 필요
* 📦 그래서 총 6개의 프레임이 필요했던 거

2. 📈 최대 프레임 수는 시스템 전체 용량
* 시스템 전체에서 사용 가능한 프레임 수는 한정되어 있다.
* 모든 프로세스가 너무 많은 프레임을 가지면 누군가는 실행하지 못하겠지?
* 그래서 할당 방법이 중요하다.

3. 🛠️ 프레임 할당 방식 (2가지)

&ensp;(1) Fixed Allocation(고정 할당)<br/>

* Equal Allocation (균등 할당)
  - 모든 프로세스에게 같은 수의 프레임을 나눠줌.
  - 예: 총 100개의 프레임이 있고 5개의 프로세스가 있다면? -> 각 프로세스에게 20개씩 나눠준다.
  - 단점: 어떤 프로세스는 메모리를 많이 필요로 하고 어떤 건 적게 필요한데 전부 똑같이 나눠주는 건 비효율적일 수 있다.

* Proportional Allocation(비례 할당)
  - 각 프로세스가 필요로 하는 메모리 크기에 비례해서 프레임을 할당
  *  공식 : $a_i = S_{s_i} \times m$
  * $s_i$ :프로세스 $p_i$ 의 메모리 크기
  * S: 전체 프로세스드르이 메모리 크기 합
  * m: 전체 프레임 수
  * $a_i$ : 프로세스 $p_i$ 에게 할당할 프레임 수

&ensp;예시: <br/>
* 총 프레임 수: 64
* 프로세스1 크기: 10
* 프로세스2 크기: 127
* 총합 S: 137

&ensp;$a_1 = 10/137 * 64 = 4$ , $a_2 = 127/137 * 64 = 60$  <br/>

&ensp;(2) Priority Allocation(우선순위 할당)<br/>
* 프로세스의 우선순위(priority)를 기준으로 프레임을 나눠주는 방식
* 높은 우선순위를 가진 프로세스는 더 많은 프레임을 받을 수 있다.

&ensp;🔄 페이지 폴트 발생 시:<br/>
1. 자기 자신의 프레임 중 하나를 교체할 수도 있고,
2. 우선순위가 낮은 프로세스의 프레임을 빼앗을 수도 있다.
&ensp;중요한(우선순위 높은) 프로세스는 더 많은 자원을 보장받도록 설계된 방식이다.<br/>

&ensp;💡요약<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-33.png" width="600"></p>

📌 Global Allocation (글로벌 할당) vs Local Allocation(로컬 할당)
------

&ensp;📌 Global Allocation (글로벌 할당)<br/>
&ensp;✅ 개념<br/>
* 모든 프로세스가 전체 메모리 프레임을 공유
* 어떤 프로세스가 페이지 교체가 필요할 때, 자기 것뿐만 아니라 다른 프로세스가 사용하는 프레임도 교체할 수 있음.

&ensp;📊 장점<br/>
* **시스템 전체 처리량(throughput)**이 높아진다.
  - 필요한 프로세스가 프레임을 더 확보해서 빠르게 작업 가능
* 실제로는 이 방식이 더 일반적으로 사용됨.

&ensp;⚠️ 단점<br/>
* 어떤 프로세스가 갑자기 프레임을 많이 가져가면, 다른 프로세스는 느려질 수 있다.
* 그래서 프로세스마다 성능이 들쑥날쑥할 수 있음 (불안정).

&ensp;📌 Local Allocation (로컬 할당)<br/>
&ensp;✅ 개념<br/>
* 각 프로세스가 받은 프레임만 사용
* 다른 프로세스의 프레임에는 절대 손대지 않음

&ensp;📊 장점<br/>
* 프로세스마다 성능이 일정하게 유지됨
  - 남이 내 메모리를 뺏어가는 일이 없음!

&ensp;⚠️ 단점<br/>
* 어떤 프로세스는 프레임이 남아도는데 다른 프로세스는 부족할 수도 있음
* 즉 메모리를 효율적으로 못 쓰는 경우가 생김(underutilized memory)

&ensp;🎯 예시로 비교<br/>
&ensp;예를 들어 프로세스 A, B가 각각 3개의 프레임을 가지고 있다고 가정

&ensp;✅ Global 할당:<br/>
* 프로세스 A가 프레임 3개로 부족해서 B의 프레임 하나를 뺏어올 수 있음 -> 전체 성능이 향상될 수 있음

&ensp;✅ Local 할당:<br/>
* A는 계속 자기 3개만 써야 함
* -> B는 프레임 남아도는데 A는 계속 페이지 폴트 발생 -> 낭비 발생

Non-Uniform Memory Access(NUMA)
------
&ensp;🔷 일반적인 메모리 접근 구조 (UMA: Uniform Memory Access)<br/>
* 모든 CPU가 메모리에 접근하는 속도가 같다.
* CPU1이든 CPU2든 메모리에 접근하는 데 걸리는 시간은 똑같음

&ensp;🔷 NUMA란?<br/>
&ensp;하지만 요즘은 컴퓨터가 더 크고 복잡해져서 다음과 같은 구조를 쓴다.<br/>
&ensp;NUMA(Non-Uniform Memory Access) : CPU마다 가까운 메모리와 먼 메모리가 있고 가까운 메모리에 접근하는 것이 훨씬 빠르다.<br/>

&ensp;예를 들어: <br/>
* CPU1은 Memory1에 가까움 -> 빠름
* CPU1은 Memory2에 접근하면 -> 느림 
&ensp;-> CPU와 메모리 사이의 거리가 성능에 영향을 준다.<br/>

&ensp;🔷 NUMA 최적화: 성능을 높이는 방법<br/>
&ensp;성능을 좋게 하려면? <br/>
* **CPU1에서 실행되는 스레드(thread)**는 CPU1과 가까운 메모리를 쓰게 해야 빨라진다다.
&ensp;그래서 운영체제는 다음과 같은 일을 한다. <br/>
1. 스레드가 어떤 CPU에서 실행될지 정하고
2. 그 CPU와 가까운 메모리를 할당해준다.

&ensp;추가 예시: 💡 Solaris의 해결책: lgroups(= CPU와 메모리를 묶은 그룹)<br/>
&ensp;운영체제인 Solaris는 이 문제를 이렇게 해결한다.<br/>
* CPU와 가까운 메모리를 함께 묶어서 그룹을 만든다. 
* 프로세스와 스레드를 가능한 한 그룹 내에서만 배치하고 메모리를 할당한다.<br/>
&ensp;이러면: 빠른 메모리 접근 가능, 전체 시스템 성능도 향상<br/>


6\. Thrashing(스래싱)
======

&ensp;✅ Thrashing이란?: 스래싱은 운영체제가 **페이지 교체 작업(swap in/out)**에 너무 많은 시간을 써서 실제 일을 거의 하지 못하는 상태를 말한다<br/>

&ensp;🧠 왜 이런 일이 생길까?<br/>
&ensp;프로세스가 충분한 페이지(프레임)를 할당받지 못할 때 생긴다. <br/>
1. 페이지가 필요하다. -> Page Fault 발생
2. 기존 프레임 하나를 교체해야 함 -> 프레임 교체
3. 그런데 곧바로 그 교체한 페이지가 다시 필요해짐
4. 또 Page Fault 발생 -> 또 교체 .. 반복
&ensp;이렇게 되면 CPU는 계산보다 페이지 들락날락 처리만 하느라 바빠지고 실제 연산은 못하게 된다.<br/>

&ensp;📉 문제가 되는 이유<br/>
&ensp;이런 현상이 반복되면: <br/>
* CPU 사용률이 급격히 떨어짐
* 운영체제는 "일이 안 되네?" 하면서 멀티프로그래밍 수준을 더 높이려고 함 -> 더 많은 프로세스를 넣음
* 오히려 상황이 더 악화됨(Page Fault 폭발)

&ensp;📊 그래프 설명<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-34.png" width="600"></p>

* x축: 동시에 실행 중인 프로세스 수 (멀티프로그래밍 정도)
* y축: CPU 활용률
* 일정 수준까지는 프로세스를 늘릴수록 CPU 사용률이 증가.
* 하지만 한계를 넘으면 오히려 급격히 떨어짐! → 이 구간이 바로 Thrashing

1. Demand Paging이 왜 잘 작동할까?
&ensp;👉 핵심 키워드: Locality Model (지역성 모델)<br/>
&ensp;**지역성(locality)**이란 프로그램이 일정 시간 동안 자주 사용하는 주소(메모리 영역)가 한정되어 있다는 개념이다. <br/>
&ensp;✅ 프로그램의 실행 방식<br/>
&ensp;프로그럄은 하나의 지역(Locality) 안에서 계속 메모리를 사용하다가 시간이 지나면 다른 지역(locality)로 이동(migrate)하면서 새로운 코드, 변수들을 사용하게 된다. 이런 지역들이 겹치기도 한다. 그래서 운영체제는 메모리에 자주 사용하는 페이지들만 잠깐 올려두면 되니까 필요할 때 로딩하는 demand paging이 효과적이다. <br/>

2. 그럼 Thrashing(스래싱)은 왜 생겨?
&ensp;👉 문제: 모든 지역(Locality)의 크기를 더했더니 메모리보다 크다!
&ensp;여러 프로세스가 실행되면서 지역들이 점점 커진다. 근데 시스템의 총 메모리 크기보다 이 모든 지역의 크기 합이 더 크다면? 결국 메모리에 모든 지역을 유지할 수 없게 되고 계속 페이지를 불러오고 내보내야 하는 상황이 생긴다. -> 이게 바로 thrashing 상태<br/>

&ensp;💡 해결 방법은?<br/>
&ensp;✅ Local 또는 Priority Page Replacement 사용<br/>
* Local Replacement: 각 프로세스는 자기 프레임만 교체하게 해서 다른 프로세스에 피해 안 주기
* Priority Replacement: 중요한 프로세스에 더 많은 메모리를 줘서 덜 중요한 애의 페이지를 교체하게 하기

Working Set Model
------
&ensp;🔍 Working Set Model이란?: 프로그램이 실행될 때 지속적으로 사용하는 메모리 페이지 집합이 있다. 이걸 Working Set이라고 한다. 한 프로세스가 최근에 접급한 페이지들의 집합이 바로 Working Set이다.<br/>

&ensp;📏 Δ(델타)는 무엇인가요?<br/>
* Δ는 '작업 집합 윈도우'를 의미한다. 
* 예를 들어 최근 10,000개의 명령어 동안 접근한 페이지들을 본다면 그게  Δ = 10,000이다.
* 이 기간 동안 얼마나 많은 페이지를 접근했는가를 측정해 Working Set을 계산한다. 
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-35.png" width="600"></p>

&ensp;만약 Δ = 10이라고 가정하면 시간 t1에서의 작업 집합은 {1, 2, 5. 6. 7}이 되고 시간 t2에서는 {3, 4}가 된다. <br/>
&ensp;작업 집합의 정확도는 Δ의 선택에 따라 좌우된다. 만약에 Δ값이 너무 작으면 전체 지역을 포함하지 못할 것이고 Δ값이 너무 크면 여러 지역성을 과도하게 수용할 것이다. 극단적으로 Δ가 무한히 크면 이 집합은 프로세스가 실행 중에 만난 모든 페이지의 집합이 된다. 그렇다면 이 모델의 가장 중요한 요소는 집합의 크기이다.<br/>

&ensp;💣 왜 이게 중요하냐면?<br/>
* **총 Working Set크기(D)**가 시스템의 물리 메모리보다 커지면 -> thrashing 발생

&ensp;🧠 Thrashing 이란?: 프로세스들이 너무 많은 페이지를 요구해서 계속 메모리에서 페이지를 넣었다 뺐다하느라 실제 계산을 못하고 바쁘기만 한 상태<br/>

&ensp;🔧 해결 방법: Working Set 기반 정책<br/>
* D > m (D는 총 작업 집합 크기, m은 메모리 크기)이면: 
  - 일부 프로세스를 잠깐 멈추거나 스왑 아웃시킴.

&ensp;🔄 Working Set을 실제로 어떻게 추적할까요?<br/>
&ensp;방법 <br/>
* 각 페이지마다 reference bit(참조 비트)를 둔다
* 일정 시간마다 타이머 인터럽트가 발생해서 
  - 페이지가 사용됐으면 reference bit를 1로 설정
  - 사용 안 됐으면 0으로 유지
* 이 reference bit들을 보고 페이지가 Working Set에 포함되는지 판다

&ensp;🎯 이 방법의 한계:<br/>
* 정확하게 Δ 동안 어떤 페이지를 썼는지 알 수는 없다.
* 예를 들어 reference bit가 1이면 최근에 썼구나정도만 알 수 있다.
* 그래서 더 정확하게 하려면 -> 여러 비트와 짧은 인터럽트 주기를 사용한다.

Page-Fault Frequency
-------

&ensp;Page-Fault Frequency (PFF): 페이지 결함 빈도 기반 프레임 조절<br/>
* 페이지 결함이 너무 자주 일어나면 프로그램이 느려지고, 너무 적게 일어나면 메모리를 낭비할 수 있다.
* PFF는 페이지 결함이 발생하는 빈도를 보고 그에 맞게 프레임 수를 늘리거나 줄이는 방법

&ensp;작동원리:<br/>
1. Acceptable Rate(허용 가능한 결함률) 을 정함
  - 예: 너무 자주도 안 되고 너무 안 일어나도 안 되는 ‘적당한 기준선’을 설정
2. 실제 페이지 결함률과 비교해서 조절
  - 📉 페이지 결함률이 낮다 -> 너무 많은 프레임을 갖고 있음 -> 프레임을 줄여도 됨
  - 📈 페이지 결함률이 높다 -> 메모리가 부족함 -> 더 많은 프레임 필요

<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-36.png" width="600"></p>

&ensp;📊 그래프 설명:<br/>
* x축: 할당된 프레임 수
* y축: 페이지 결함률
* 프레임 수가 많아질수록 결함률이 줄어들지만 너무 적으면 급격히 증가함
* 상한(Upper bound) 넘으면 프레임을 늘림
* 하한(Lower bound) 밑이면 프레임을 줄임

&ensp;Working Sets and page Fault Rates: 위킹 셋과 페이지 결함률의 관계<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-37.png" width="600"></p>

* working set: 한 시점에 실제로 자주 사용하는 페이지들의 집합
* 프로그램은 항상 모든 메모리를 사용하는 게 아니라 일정 시간에 일부 페이지만 집중적으로 사용한다. 

&ensp;🧠 작동 방식:<br/>
* 시간에 따라 프로그램이 필요한 페이지가 계속 바뀜 -> 시간이 지나면 워킹 셋이 바뀌고 그때 새로운 페이지들을 불러와야 해서 페이지 결함이 발생

&ensp;그래프 설명<br/>
* x축: 시간
* y축: 페이지 결함률
* 그래프는 올랐다가 내려가고 반복하는데 이는 위킹 셋이 바뀌며 새로운 페이지가 필요할 때 마다 결함이 생긱기 때문이다. 
* 위킹 셋에 맞는 프레임을 적절히 할당하지 않으면 Thrashing이 발생할 수 있다.

💾 Memory-Mapped Files란?
-------
&ensp;메모리 매핑 파일이란 파일 입출력을 메모리 접근처럼 처리하는 방식이다. 보통 파일을 읽거나 쓸 때는 read()나 write() 같은 시스템 콜을 써야 한다. 그런데 메모리 매핑을 하면 이런 입출력을 메모리에 직접 전근하듯이 할 수 있다.<br/>

&ensp;🧠 기본 개념<br/>
&ensp;✅ 매핑(mapping)한다는 건?<br/>
&ensp;디스크에 있는 파일의 일부를 가상 메모리의 특정 주소에 연결(연결시켜서 매핑)한다는 뜻다. 마치 디스크에 있는 내용을 메모리처럼 다룰 수 있게 되는 거다. 실제로는 물리 메모리와 디스크 사이에 페이지 단위로 이동이 일어난다.(-> demand paging 사용)

&ensp;🧱 동작 방식 요약<br/>
1. 프로그램이 파일을 열 때 이부 페이지를 메모리에 매핑함(map() 시스템 콜 사용)
2. 읽거나 쓸 때 일반 메모리 접근처럼 *(ptr)을 통해 데이터에 접근함
3. 처음 접근하는 순간, 페이지 폴트 발생 → OS가 디스크에서 해당 페이지를 메모리로 불러옴
4. 수정된 페이지는 OS가 나중에 디스크로 저장함 (close() 호출 시나 주기적으로 저장)

&ensp;📌 장점<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-38.png" width="600"></p>

&ensp;🔄 언제 디스크에 실제로 저장돼?<br/>
* 파일을 close()하거나 OS가 dirty page(수정된 페이지)를 찾아 저장할 때 또는 주기적으로 백그라운드 작업을 통해 디스크에 저장함

&ensp;🛠 시스템 콜 예시<br/>
* mmap() : 파일을 메모리에 매핑, munmap() : 매핑 해제, msync() : 매핑된 페이지를 디스크로 저장

<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-39.png" width="600"></p>

&ensp;서로 다른 프로세스 A와 B가 같은 디스크 파일의 내용을 각각 자신의 메모리에 매핑해서 공유하고 있음

&ensp;Shared Memory via Memory-Mapped I/O<br/>

<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-40.png" width="600"></p>

&ensp;📌 왼쪽 프로세스 (process₁)<br/>
* shared memory라는 블록이 하나 존재
* 이 메모리는 중앙에 있는 memory-mapped file을 가리킨다.
&ensp;📌 가운데 블록 (memory-mapped file)<br/>
* 디스크의 파일이 실제 메모리에 매핑된 모습
*  메모리를 여러 프로세스가 동시에 참조
&ensp;📌 오른쪽 프로세스 (process₂)<br/>
* 이 프로세스도 동일하게 shared memory를 통해 중앙의 메모리를 접근

&ensp;🔄 동작 방식 요약<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-41.png" width="600"></p>
&ensp;-> 파일을 직접 열고 쓰지 않고도 메모리처럼 빠르게 읽고 쓸 수 있는 장점이 있다.<br/>

&ensp;💻 윈도우에서 사용하는 API 방식 예시<br/>
1. CreateFile()로 공유할 파일을 연다.
2. CreateFileMapping()으로 파일을 공유 메모리 객체로 매핑한다.
3. MapViewOfFile()로 해당 파일을 프로세스의 가상 주소 공간에 연결한다.

7\. 커널 메모리의 할당(Allocating Kernel Memory)
======

&ensp;🧠 커널 메모리 할당이란?<br/>
&ensp;우리가 사용하는 앱이나 프로그램이 CPU와 메모리 같은 하드웨어를 직접 제어할 수 없기 때문에 커널이 대신 도와주는 중간 관리자 역할을 한다. 이 커널도 작동을 위해 메모리에 필요하다. 이때 사용하는 메모리를 커널 메모리라고 한다.<br/>

&ensp;📌 사용자 메모리와는 다르게 다룸<br/>
* 우리가 프로그램을 실행하면 일반적으로 "사용자 메모리(user memory)"가 할당된다.
* 하지만 커널은 더 높은 권한이 필요한 작업을 하기 때문에 별도의 커널 메모리를 따로 관리
* 즉, 커널이 쓰는 메모리와 일반 사용자가 쓰는 메모리는 분리돼 있다.

&ensp;📦 커널 메모리는 어디서 오냐?<br/>
* 커널은 보통 free-memory pool(자유 메모리 폴)에서 메모리를 가져온다.
* 이 풀은 운영체제가 관리하는 비어 있는 메모리 조각 모음이다.

&ensp;🧩 커널이 요청하는 메모리는 다양<br/>
&ensp;커널이 요청할 때 운영체제는 크기 맞춰서 알맞은 메모리를 골라서 줘야 한다.<br/>

&ensp;🧷 일부 커널 메모리는 연속된 메모리가 필요<br/>
&ensp;예를 들어, 디바이스 I/O(입출력)를 위한 메모리!<br/>
&ensp;예시) 하드디스크에 파일을 저장할 때<br/>
* 데이터를 디스크로 한 번에 보내려면 연속된 메모리 공간이 필요, 만약 조각조각 흩어져 있다면 I/O 장치가 처리하기 힘듦

Buddy System
------

&ensp;✅ 1. 버디 시스템의 핵심 개념<br/>
&ensp;✔ 메모리 블록을 "2의 제곱" 크기로 나눔<br/>
* 예: 2KB, 4KB, 8KB, 16KB, 32KB, … 이런 식
* 요청한 크기보다 큰 2의 제곱 블록을 할당함 (예: 21KB 요청 -> 32KB 할당)
&ensp;✔ 더 작은 블록이 필요하면 계속 반으로 쪼개기<br/>
* 256KB 블록 → 128KB → 64KB → 32KB …
* 나눈 두 블록을 버디(buddy) 라고 부름

&ensp;✅ 2. 동작 예시
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-42.png" width="600"></p>

&ensp;예: 커널이 21KB를 요청한다고 가정할 때<br/>
1. 256KB 블록이 사용 가능함
2. 이걸 반으로 나눔 → 128KB / 128KB(A<sub>L</sub>, A<sub>R</sub>)
3. 왼쪽 블록 A<sub>L</sub>을 또 나눔 → 64KB / 64KB (B<sub>L</sub>, B<sub>R</sub>)
4. B<sub>L</sub>을 또 나눔 → 32KB / 32KB (C<sub>L</sub>, C<sub>R</sub>)
5. 드디어 32KB가 나왔고, 21KB 요청은 이걸로 충족됨

&ensp;✅ 3. 장점 👍<br/>
* 빠르다! -> 나누거나 병합하는 과정이 간단함
* 물리적으로 연속된 메모리 확보 가능 -> 디바이스 I/O 같은 경우 연속된 메모리가 필요할 때 유리

&ensp;❗ 단점 👎<br/>
* 외부 단편화(External Fragmentation) : 예를 들어 21KB를 위해 32KB를 썼지만 11KB는 못 씀

&ensp;🔄 반대로 병합도 가능!: 만약 두 개의 버디 블록이 모두 사용되지 않았으면 다시 합쳐서 큰 블록으로 되돌릴 수 있다.<br/>

Slab Allocator 
-----

&ensp;Slab Allocator는 운영체제가 **자주 사용하는 커널 객체들(구조체 등)**을 빠르고 효율적으로 할당/해제하기 위해 사용하는 메모리 관리 기법<br/>

&ensp;💡 용어 정리<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-43.png" width="600"></p>

&ensp;🎯 작동 원리<br/>
1. 어떤 구조체가 필요하면
  - Cache에서 Slab을 찾아서
  - Free된 object가 있으면 그걸 반환
2. 만약 다 찼다면?
  - 빈 Slab에서 가져옴
  - 그것도 없으면 새 Slab을 만들어서 사용

&ensp;🔁 Slab 상태는 3가지<br/>
* Full: Slab안에 있는 object들이 전부 사용됨
* Empty: 전부 Free 상태
* Partial: 일부만 사용 중, 일부는 Free 

&ensp;장점<br/>
<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-44.png" width="600"></p>

<p align="center"><img src="/assets/img/Operating System/10. virtual memory/10-45.png" width="600"></p>

1. Kernel Objects
* 커널에서 사용하는 자료구조
  - 이 그림에서는 두 가지 크기의 객체가 있다: 3KB짜리 객체들, 7KB짜리 객체들

2. Caches (캐시)
* 각 객체 유형마다 전용 캐시가 있어. 즉, 3KB 객체만 저장하는 캐시, 7KB 객체만 저장하는 캐시가 따로 있음.
* 이 캐시는 같은 종류의 객체들을 Slab에서 빠르게 꺼내 쓸 수 있도록 관리하는 중간 계층
* 객체를 요청하면 캐시는 다음과 같이 반응한다:
  1. 이미 만들어진 Free 객체가 있으면 그것을 제공
  2. 없으면 Slab에서 새로운 걸 할당
  3. 그래도 없으면 Slab을 새로 만들어 사용

3. Slabs (슬랩)
* Slab은 실제로 메모리 공간이야. 여러 개의 객체를 담을 수 있는 연속된 물리적 페이지들로 구성됨
* 캐시는 슬랩을 여러 개 가질 수 있어.
* 한 슬랩 안에는 같은 크기의 객체만 담김 (3KB 슬랩에는 3KB 객체만, 7KB 슬랩에는 7KB 객체만)

&ensp;🔁 작동 방식 정리<br/>
1. 커널이 어떤 구조체 객체를 할당해달라고 요청함
2. 캐시가 그 크기에 맞는 슬랩에서 **비어있는 객체(free object)**를 찾아서 반환
3. 만약 슬랩에 남은 객체가 없다면:
  - 새로운 슬랩을 할당
  - 그 안에서 객체를 반환
4. 해제된 객체는 free 상태로 남아서 다시 재사용될 수 있음


&ensp;🧠 Linux에서 Slab 할당 예시<br/>

```cpp
struct task_struct; // 커널에서 프로세스를 나타내는 구조체

// Slab Cache는 struct task_struct만 저장
```

1. Slab Allocator는 struct task_struct 캐시에서 사용하지 않은 걸 찾아줌
2. Partial Slab → 없으면 Empty Slab → 없으면 New Slab 생성

&ensp;🐧 리눅스에서의 발전 과정<br/>
&ensp;✅ SLAB (기본 슬랩)<br/>
* 커널에서 데이터 구조를 위한 **슬랩(캐시된 객체들)**을 관리
* free, used 상태를 나누고, 효율적인 재사용 가능

&ensp;✅ SLOB (Simple List Of Blocks)<br/>
* 이름 그대로 아주 단순한 구조의 슬랩 할당자
* **작은 시스템 (메모리 적은 임베디드 디바이스)**에 적합해
* 어떻게 관리하냐면?
  - 작은 블록 리스트, 중간 크기 블록 리스트, 큰 블록 리스트 이렇게 3개의 리스트로 메모리를 분류해서 관리해

&ensp;✅ SLUB (SLAB + 성능 최적화)<br/>
* SLAB보다 더 빠르고 성능 좋은 슬랩 할당자
* CPU별 큐를 제거함 → 구조가 단순해짐
* **메타데이터(상태 정보 등)**를 페이지 구조 안에 같이 저장함 → 공간 절약
* 대부분의 리눅스 시스템에서 기본값으로 사용됨

8\. Other Issues
======

1. Prepaging
&ensp;프리페이징이란: 필요할지 아직 모르지만 미리 책장을 펴두는 것과 같다.<br/>
&ensp;📚 실제 생활 예시: 🎒 너가 학교에 갈 때를 생각해보자!<br/>
* 수업에서 필요할지도 몰라서 책 10권을 미리 가방에 넣어 갔다.
* 그런데 실제로 수업에서 쓴 건 딱 2권뿐
* 나머지 8권은 그냥 들고 다니기만 함
&ensp;-> 이게 바로 프리페이징이 실패한 경우이다.<br/>

&ensp;💻 컴퓨터에서는? <br/>
&ensp;컴퓨터에서 프로그램을 실행할 때도 많은 메모리를 쓴다. 근데 첨음에 어떤 메모리를 쓸지 몰라서 미리 여러 페이지를 로딩할 수 있다. 이걸 프리페이징이라고 한다.<br/>
&ensp;장점은?: 미리 불러놨으니까 나중에 찾을 필요가 없다.(빠르다.)<br/>
&ensp;단점은?: 안 쓸 페이지까지 미리 불러오면 시간과 메모리 낭비가 생긴다.<br/>

* s : 미리 불러온 페이지 수 (책을 몇 권 들고 갔냐?)
* α (알파) : 그 중 실제로 쓴 페이지 비율 (책을 실제로 몇 권 읽었냐?)

&ensp;계산해보면: <br/>
* 유용했던 수: s * α → 사용된 페이지 수
* 쓸모없던 수: s * (1 - α) → 괜히 불러온 페이지 수

&ensp;결론<br/>
* α (알파)가 크면?: 대부분의 페이지를 실제로 썼다 -> 프리페이징 성공
* α가 작으면?: 대부분 안 썼다 -> 프리페이징 실패

2. Page size
* 운영체제 설계자가 페이지 크기를 선택할 수 있는 경우가 있다.
* 고려해야 할 요소
  - 조각화(Fragmentation): 페이지가 너무 크면 낭비 발생 가능
  - 페이지 테이블 크기: 페이지가 작으면 페이지 테이블이 커짐
  - 입출력 부담, 페이지 결함 수, 지역성 등도 고려
  - TLB와 관련된 성능도 영향
* 보통 2의 거듭제곱 크기 사용
* 평균적으로 점점 더 큰 페이지가 선호되는 추세

3. I/O Interlock
* I/O 수행 시 페이지가 메모리에 고정(pinning) 되어야 함.
* 디스크에서 파일을 복사할 때 사용하는 페이지는 교체 대상이 되면 안 됨.
* 해결 방법:
  - 해당 페이지를 교체 알고리즘에서 제외 -> I/O interlock
  - 핀(Pinning)을 통해 해당 페이지가 메모리에 유지되도록 함

9\. Operating System Examples
=======

&ensp;Windows의 메모리 관리 정책
* 클러스터링(Clustering): 결함 발생 시 주변 페이지도 같이 가져옴
* 각 프로세스는 다음 두 값을 가짐
  - working set minimum : 최소 보장 페이지 수
  - working set maximum : 최대 허용 페이지 수
* 자동 작업 세트 트리밍: 메모리가 부족하면 일부 프로세스에서 페이지를 강제로 회수하여 전체 메모리를 확보함
