---
title: "13주차 차원 줄이기"
excerpt: ""

wirter: sohee kim
categories:
  - Machine Learning
tags:
  - Machine Learning

toc: true
toc_sticky: true
use_math: true
  
date: 2025-06-04
last_modified_at: 2025-06-06
---

차원 줄이기(Dimensionality Reduction)
=======

&ensp;현실의 데이터는 단순히 두세 개 숫자가 아니라, 수십 개의 특징(Feature) 값들로 이루어진 고차원 벡터이다.<br/>
&ensp;예를 들어:<br/>
* 자동차의 성능을 나타내는 20가지 특징이 있다면 → 데이터는 20차원 벡터로 표현됨.

&ensp;이렇게 차원이 높아지면 생기는 문제점<br/>
* 계산 시간이 길어진다! (컴퓨터가 느려짐)
* 사람이 눈으로 보기 어렵다! (그래프 표현 불가)

&ensp;그래서 데이터를 간단하게, 정보는 최대한 유지하면서 차원을 줄이는 기술이 필요하다<br/>

&ensp;차원 줄이기의 목적 2가지<br/>
<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-1.png" width="600"></p>

&ensp;데이터 압축<br/>
&ensp;데이터에는 중복된 정보(Redundancy)가 많다.<br/>

&ensp;예시 1: cm와 inch<br/>
* 어떤 물체의 길이를 cm와 inch로 저장 -> 두 값은 사실상 같은 정보
* 동일한 길이이므로 두 특징 값은 매우 높은 상관관계 유지(즉 중복)

&ensp;이런 경우, 두 개의 특징 대신 하나의 축으로 줄일 수 있다.<br/>
<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-2.png" width="600"></p>

&ensp;예시 2: 조종사의 실력(skill) vs 열정(enjoyment)<br/>
* 실력이 높아지면 비행을 더 좋아하게 되고 → 서로 상관관계 높음
* 두 개 특징 → 하나로 합쳐서 ‘적성(aptitude)’이라고 볼 수 있다.

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-3.png" width="600"></p>

&ensp;요점: 서로 비슷한 정보를 가진 특징들은 하나로 합쳐도 괜찮다.(즉 불필요한 redundancy를 줄이기 위해 2개의 매우 높은 상관관계를 보이고 있는 특징 값을 하나의 특징 값으로 변환)<br/>
&ensp;이렇게 하면 차원을 줄이면서도 정보 손실을 최소화할 수 있다. <br/>

&ensp;투영(Projection)<br/>
&ensp;고차원 데이터를 낮은 차원으로 줄일 때, 데이터들을 새로운 축(z₁, z₂ 등) 위로 “투영”시킨다. <br/>

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-4.png" width="600"></p>

&ensp;데이터 시각화<br/>
&ensp;50차원 벡터 같은 고차원 데이터는 그래프로 보기 어렵다.<br/>

&ensp;예시: 나라별 데이터<br/>
* South Korea: GDP, 인구, 기대수명, 교육지수 등… 50가지 지표!
* 이런 데이터는 50차원 벡터로 표현 -> 그래프로 보기 힘듦

&ensp;그래서 고차원을 2차원으로 줄여서 \(z1, z2\)형태로 각 나라를 점 하나로 표현 할 수 있다. <br/>

&ensp;중요한 점<br/>
* 줄인 차원의 의미 \(z1, z2\)는 해석이 어려움
* z₁, z₂는 단순히 시각화 목적이지 각각 GDP나 인구를 의미하지 않는다. 

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-5.png" width="600"></p>

Pricipal Component Analysis
======

&ensp;PCA(Pricipal Component Analysis): 높은 차원 데이터의 차원을 줄이기 위해 낮은 차원의 평면(방향)을 찾아내어 데이터를 Projection하는 것<br/>

&ensp;PCA의 목적: 고차원 데이터를 요약하면서 정보 손실은 최소화하는 것<br/>
* 100차원 -> 2차원
* 50차원 -> 1차원

&ensp;차원을 줄여서 데이터의 핵심만 남기고 불필요한 중복 정보를 없애는 기법이다.<br/>

&ensp;<b>PCA를 하기 전에 꼭 해야 하는 두 가지 전처리</b><br/>

1. 평균 정규화 (Mean Normalization)

&ensp;모든 특징의 평균을 0으로 맞춤 -> 데이터의 중심을 원점으로<br/>

2. Feature Scaling (특징 스케일 조정)

&ensp;단위가 다른 특징들(예: 키 cm vs 나이)은 동일한 범위로 맞춰줘야 함<br/>
&ensp;예: 키 170, 나이 20 -> 둘 다 0~1 범위로 정규화<br/>

&ensp;이 두 가지 과정을 거쳐야 공정하게 중요한 방향을 찾을 수 있다. 

&ensp;예: 2차원 데이터 (x₁, x₂)를 1차원으로 줄이기<br/>
* 많은 점들이 흩어져 있는 2차원 평면에서, 이 점들을 가장 잘 요약할 수 있는 직선 방향을 찾아야 한다. 

* 이걸 1차원으로 줄이려면 
  - 하나의 직선을 찾고
  - 데이터를 그 직선 위에 투영시킨다. -> 데이터는 1차원 점들로 변환된다.

&ensp;좋은 Projection vs 나쁜 Projection<br/>

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-6.png" width="600"></p>

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-7.png" width="600"></p>

&ensp;좋은 투영: Projection 오차가 작다 = 원래 점에서 투영된 점까지의 거리(수직선)가 짧다.<br/>
&ensp;나쁜 투영: 데이터와 직선 사이 거리가 멀다 = 투영 오차가 큼<br/>

&ensp;PCA는 Projection 오차의 제곱합을 최소화하는 방향을 찾는 것!<br/>

&ensp;수학적으로 보면?<br/>
* n차원 -> k차원 줄이려면:
  - 서로 직교하는 k개의 벡터 (u¹, u², ..., uᵏ) 를 찾아야 한다.
  - 각각의 데이터 x를 이 k개의 축 위로 투영해서 z 벡터를 얻는다.
* 예: 3차원 벡터 (x₁, x₂, x₃) → 2차원 벡터 (z₁, z₂)

&ensp;PCA vs 선형 회귀(Linear Regression)의 차이<br/>
<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-8.png" width="600"></p>

&ensp;PCA는 "데이터를 요약"하려는 것이고 회귀는 무언가를 예측하려는 거다.<br/>

&ensp;요약<br/>
<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-9.png" width="600"></p>


PCA 알고리즘
======

&ensp;PCA 알고리즘의 목적<br/>
&ensp;고차원 데이터에서 중요 정보만 남기고 차원을 줄이는 알고리즘이다.<br/>
* 예: 사진, 센서, 문서 등의 데이터는 수 백 ~ 수 천 차원
* PCA는 이 중에서 가장 중요한 방향을 찾아서 요약해준다. 

1. 데이터 전처리(Preprcessing)

* 평균 정규화(Mean Normalization)

&ensp;각 특징(feature)의 평균을 0으로 맞추기<br/>
&ensp;$x^{(i)}_j \leftarrow x^{(i)}_j - μ_j$ <br/>

&ensp;$\mu_j = \frac{1}{m} \sum_{i=1}^{m} x_j^{(i)}$ <br/>
&ensp;-> 데이터의 중심을 원점으로 이동시킨다.(중심 정렬)<br/>

* 특징 스케일링(Feature Scaling)

&ensp;특징들마다 값의 단위/범위가 다르면 비교가 어렵다. 그래서 비슷한 범위로 맞춰야 한다. <br/>
&ensp;$x^{(i)}_j \leftarrow \frac{x^{(i)}_j - \mu _j}{s_j}$ <br/>

&ensp;$s_j$ : 각 특징위 표준편차 또는 max-min 범위<br/>

2. 공분산 행렬 계산(Covariance Matrix)

&ensp;데이터 간의 상관관계를 담은 행렬이다. <br/>
&ensp;계산 방법<br/>
* 데이터 행렬 X 크기: n x m (n = 특징 수, m = 샘플 수)
* 공분산 행렬 Σ 크기: $\Sigma = \frac{1}{m}XX^{T}$ 

&ensp;특징<br/>
* n x n 정방행렬(Square Matrix)
* 항상 대칭(Symmetric)

3. 특이값 분해 (SVD)로 고유벡터 구하기

&ensp;공분산 행렬에서 가장 중요한 방향을 찾기<br/>

* 매트랩 코드: [U,S,V]=svd(Σ)

* 여기서 U 행렬은 고유벡터(주성분)들이 들어있는 행렬

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-10.png" width="600"></p>

4. 주성분 선택(축소할 차원 선택)
* 원래 U는 n x n 행렬
* 앞에서부터 k개의 열만 선택해서 $U_{reduce}$ 생성

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-11.png" width="600"></p>

&ensp;여기서 k < n 이고, 줄이고 싶은 차원 수<br/>

5. 데이터 차원 축소(Projection)

&ensp;데이터 벡터 $x \in R^{n}$ 를 k차원 벡터 $x \in R^{n}$ 로 투영<br/>

&ensp;$z = U^T_{reduce}x$ <br/>

* z: 압축된 데이터(새로운 좌표)
* $U^T_{reduce} : k \times n$ -> z는 k x 1 벡터가 됨 

&ensp;요약<br/>
<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-12.png" width="600"></p>

&ensp;PCA 알고리즘 핵심 요약<br/>
<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-13.png" width="600"></p>


Principal Component 수의 결정
======

1. k(주성분 개수) 결정<br/>
&ensp;PCA는 차원 축소를 위해 데이터를 요약하는 거다. 그런데 너무 적게 요약하면 중요한 정보를 잃고, 너무 많이 요약하면 차원 축소한 의미가 없다. <br/>
&ensp;그래서 **최적의 k값** 을 찾아야 한다. <br/>

2. PCA에서 오차(Error)란?<br/>
&ensp;원래 데이터와 복원한 데이터 사이의 차이를 말한다. <br/>
&ensp;예를 들어: 원래 2차원 데이터를 1차원으로 압축했다가 다시 복원하면 복원된 데이터는 원래 데이터와 완전히 같지 않다. (정보 일부가 사라짐) 그 차이를 오차라고 한다. <br/>

&ensp;오차 계산 공식<br/>

* 평균 제곱 투영 오차: $\frac{1}{m} \sum_{i=1}^{m} \left\| x^{(i)} - x_{\text{approx}}^{(i)} \right\|^2$ 

* $x^{(i)}$ : 원래 데이터
* $x^{(i)}_{approx}$ : 복원된 근사 데이터

3. 분산(Total Variation)
* 분산은 데이터가 퍼져 있는 정도
* 모든 데이터의 크기를 제곱해서 평균 낸 걸 말한다.

&ensp;$\text{Total Variation} = \frac{1}{m} \sum_{i=1}^{m} \left\| x^{(i)} \right\|^2$ <br/>
&ensp;데이터가 많이 퍼져 있을수록 분산이 크다.<br/>

4. 어떤 기준으로 k를 선택할까?

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-14.png" width="600"></p>

* 이 비율이 0.01 이하가 되면, 전체 정보 중에서 99% 이상 보존한 거예요.
* 그러면 이때의 k가 적당하다고 판단한다.

&ensp;예:<br/>
* 0.05 → 95% 보존
* 0.10 → 90% 보존

5. 반복해서 k 찾기 (비효율적인 방법)
&ensp;1. k = 1부터 시작<br/>
&ensp;2. PCA 실행해서 압축 후 복원<br/>
&ensp;3. 위 비율 계산<br/>
&ensp;4. 조건 만족하면 멈춤, 아니면 k += 1<br/>
&ensp;하지만 이건 매번 PCA 계산을 반복해야 하므로 비효율적이다.<br/>

6. 더 효율적인 방법: S 행렬 사용
&ensp;PCA에서는 SVD(Singular Value Decomposition)라는 걸 쓴다:
&ensp;$\Sigma = \frac{1}{m} X^T X \xrightarrow{\text{SVD}} [U, S, V]$ <br/>

* 여기서 S 행렬은 대각선 요소만 있는 행렬로, 이 대각 원소 하나하나가 데이터의 분산을 의미

&ensp;비율을 S로 바로 계산<br/>

&ensp;$\text{보존 비율} = \frac{s_1 + s_2 + \cdots + s_k}{s_1 + s_2 + \cdots + s_n}$<br/>
* 이 비율이 0.99 이상이면 k를 선택한다.
* 계산도 빠르고 효율적이댜.

7. 데이터 복원 과정도 다시 보기

&ensp;압축된 데이터 z를 다시 복원하려면: $x_{approx} = U_{reduce} \cdot z$ <br/> 

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-15.png" width="600"></p>

* $U_{reduce}$ : 주성분 방향 벡터들
* z: 낮은 차원의 벡터 (압축된 정보)
* 결과: 원래 차원으로 복원된 근사 데이터

&ensp;요약정리<br/>
<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-16.png" width="600"></p>

PCA 적용 방법
======

&ensp;<b>1. PCA의 적용 목적</b><br/>

&ensp;PCA는 차원 줄이기(Dimensionality Reduction)를 위해 사용된다.<br/>
&ensp;그 목적은 크게 두 가지이다. <br/>
1. 학습 속도 향상: 입력 데이터의 차원이 줄면 계산량이 줄어든다.
2. 시각화: 데이터를 2D, 3D로 표현할 수 있게 된다.

&ensp;<b>2. 예시: 이미지 분류에서의 PCA 적용</b><br/>

&ensp;상황<br/>
* 이미지 하나가 100 × 100 픽셀이면 = 10,000개의 입력값 (10,000차원 벡터)
* 이런 고차원 데이터는 학습 속도가 느려지고, 메모리도 많이 차지한다.

&ensp;해결책<br/>
&ensp;PCA로 10,000 차원 -> 1,000차원으로 줄여서 학습에 사용함

&ensp;적용 흐름<br/>
  &ensp;1. 원래 학습 데이터<br/>

  &ensp;$(x^1, y^1), (x^2, y^2),....,(x^m, y^m)$ <br/>
  &ensp;x는 이미지 데이터, y는 라벨<br/>

  &ensp;2. PCA 적용<br/>
  
  &ensp;x만 골라서 -> z로 압축(예: 10,000 -> 1,000차원)<br/>

  &ensp;3. 새로운 학습 데이터<br/>

  &ensp;$(z^1, y^1), (z^2, y^2),....,(z^m, y^m)$ <br/>

  &ensp;4. 로지스틱 회귀 모델 학습<br/>

  &ensp;$h(z) = \frac{1}{1 + e^{- \mathbf{w}^T \mathbf{z}}}$ <br/>

  &ensp;5. 데스트 데이터도 같은 방식으로 압축 -> 예측<br/>


&ensp;<b>3. PCA 적용 수식 정리</b><br/>
* PCA는 공분산 행렬(covariance Matrix)을 구해서 가장 분산이 큰 방향을 찾는다.
* 이걸 위해 SVD(특이값 분해)를 사용한다.

&ensp;주요 수식<br/>
* X_train: m개의 학습 데이터를 행렬로 표현
* 공분산 행렬 계산: $$ \sum = \frac{1}{m}X^TX$
* SVD 수행: $\sum = U\Lambda U^T$ 
* 차원 축소: $U^T_{reduce}\cdot x$ (U_reduce는 k개 주성분을 모아 만든 행렬)

&ensp;<b>4. PCA 적용 전 필수 전처리</b><br/>

&ensp;1. 평균 정규화 (Mean Normalization)<br/>
* 평균을 0으로 맞추기: 각 특성값에서 평균값을 빼줌

&ensp;2. 스케일링(Feature Scaling)<br/>
* 특징값 간의 범위 차이를 줄여줌(예: 정규화, 표준화 등)

&ensp;<b>5. PCA의 활용 예시</b><br/>

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-17.png" width="600"></p>

* 보존할 분산의 비율(예: 95%)에 따라 주성분 개수 k를 정한다.

&ensp;<b>6. PCA의 잘못된 사용: Overfitting 방지용</b><br/>

&ensp;흔한 오해<br/>
&ensp;PCA로 특징 수 줄이면 Overfitting도 줄어들겠지<br/>

&ensp;왜 문제일까?<br/>
* PCA는 x만 보고 변환하고, y는 고려하지 않는다.
* 즉, 레이블 정보 없이 중요한 방향을 고르기 때문에, 학습에 꼭 중요한 정보를 버릴 수도 있다. 

&ensp;대신 Regularization을 써야 해요!<br/>
* 정규화 항을 넣어 오버피팅을 방지: $J(\mathbf{w}) = \text{비용함수} + \lambda \sum_i w_i^2$ 
* 또느 데이터 수 자체를 늘리는 것도 한 방법

&ensp;<b>7. 올바른 PCA 사용 절차</b><br/>
<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-18.png" width="600"></p>

&ensp;<b>정리: 언제 PCA를 쓸까?</b><br/>
<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-19.png" width="600"></p>