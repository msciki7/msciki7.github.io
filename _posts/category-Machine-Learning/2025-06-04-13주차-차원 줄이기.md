---
title: "13주차 차원 줄이기기"
excerpt: ""

wirter: sohee kim
categories:
  - Machine Learning
tags:
  - Machine Learning

toc: true
toc_sticky: true
use_math: true
  
date: 2025-06-04
last_modified_at: 2025-06-06
---

&ensp;차원 줄이기(Dimensionality Reduction)<br/>
&ensp;현실의 데이터는 단순히 두세 개 숫자가 아니라, 수십 개의 특징(Feature) 값들로 이루어진 고차원 벡터이다.<br/>
&ensp;예를 들어:<br/>
* 자동차의 성능을 나타내는 20가지 특징이 있다면 → 데이터는 20차원 벡터로 표현됨.

&ensp;이렇게 차원이 높아지면 생기는 문제점<br/>
* 계산 시간이 길어진다! (컴퓨터가 느려짐)
* 사람이 눈으로 보기 어렵다! (그래프 표현 불가)

&ensp;그래서 데이터를 간단하게, 정보는 최대한 유지하면서 차원을 줄이는 기술이 필요하다<br/>

&ensp;차원 줄이기의 목적 2가지<br/>
<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-1.png" width="600"></p>

&ensp;데이터 압축<br/>
&ensp;데이터에는 중복된 정보(Redundancy)가 많다.<br/>

&ensp;예시 1: cm와 inch<br/>
* 어떤 물체의 길이를 cm와 inch로 저장 -> 두 값은 사실상 같은 정보
* 동일한 길이이므로 두 특징 값은 매우 높은 상관관계 유지(즉 중복)

&ensp;이런 경우, 두 개의 특징 대신 하나의 축으로 줄일 수 있다.<br/>
<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-2.png" width="600"></p>

&ensp;예시 2: 조종사의 실력(skill) vs 열정(enjoyment)<br/>
* 실력이 높아지면 비행을 더 좋아하게 되고 → 서로 상관관계 높음
* 두 개 특징 → 하나로 합쳐서 ‘적성(aptitude)’이라고 볼 수 있다.

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-3.png" width="600"></p>

&ensp;요점: 서로 비슷한 정보를 가진 특징들은 하나로 합쳐도 괜찮다.(즉 불필요한 redundancy를 줄이기 위해 2개의 매우 높은 상관관계를 보이고 있는 특징 값을 하나의 특징 값으로 변환)<br/>
&ensp;이렇게 하면 차원을 줄이면서도 정보 손실을 최소화할 수 있다. <br/>

&ensp;투영(Projection)<br/>
&ensp;고차원 데이터를 낮은 차원으로 줄일 때, 데이터들을 새로운 축(z₁, z₂ 등) 위로 “투영”시킨다. <br/>

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-4.png" width="600"></p>

&ensp;데이터 시각화<br/>
&ensp;50차원 벡터 같은 고차원 데이터는 그래프로 보기 어렵다.<br/>

&ensp;예시: 나라별 데이터<br/>
* South Korea: GDP, 인구, 기대수명, 교육지수 등… 50가지 지표!
* 이런 데이터는 50차원 벡터로 표현 -> 그래프로 보기 힘듦

&ensp;그래서 고차원을 2차원으로 줄여서 \(z1, z2\)형태로 각 나라를 점 하나로 표현 할 수 있다. <br/>

&ensp;중요한 점<br/>
* 줄인 차원의 의미 \(z1, z2\)는 해석이 어려움
* z₁, z₂는 단순히 시각화 목적이지 각각 GDP나 인구를 의미하지 않는다. 

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-5.png" width="600"></p>

Pricipal Component Analysis
======

&ensp;PCA(Pricipal Component Analysis): 높은 차원 데이터의 차원을 줄이기 위해 낮은 차원의 평면(방향)을 찾아내어 데이터를 Projection하는 것<br/>

&ensp;PCA의 목적: 고차원 데이터를 요약하면서 정보 손실은 최소화하는 것<br/>
* 100차원 -> 2차원
* 50차원 -> 1차원

&ensp;차원을 줄여서 데이터의 핵심만 남기고 불필요한 중복 정보를 없애는 기법이다.<br/>

&ensp;<b>PCA를 하기 전에 꼭 해야 하는 두 가지 전처리</b><br/>

1. 평균 정규화 (Mean Normalization)

&ensp;모든 특징의 평균을 0으로 맞춤 -> 데이터의 중심을 원점으로<br/>

2. Feature Scaling (특징 스케일 조정)

&ensp;단위가 다른 특징들(예: 키 cm vs 나이)은 동일한 범위로 맞춰줘야 함<br/>
&ensp;예: 키 170, 나이 20 -> 둘 다 0~1 범위로 정규화<br/>

&ensp;이 두 가지 과정을 거쳐야 공정하게 중요한 방향을 찾을 수 있다. 

&ensp;예: 2차원 데이터 (x₁, x₂)를 1차원으로 줄이기<br/>
* 많은 점들이 흩어져 있는 2차원 평면에서, 이 점들을 가장 잘 요약할 수 있는 직선 방향을 찾아야 한다. 

* 이걸 1차원으로 줄이려면 
  - 하나의 직선을 찾고
  - 데이터를 그 직선 위에 투영시킨다. -> 데이터는 1차원 점들로 변환된다.

&ensp;좋은 Projection vs 나쁜 Projection<br/>

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-6.png" width="600"></p>

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-7.png" width="600"></p>

&ensp;좋은 투영: Projection 오차가 작다 = 원래 점에서 투영된 점까지의 거리(수직선)가 짧다.<br/>
&ensp;나쁜 투영: 데이터와 직선 사이 거리가 멀다 = 투영 오차가 큼<br/>

&ensp;PCA는 Projection 오차의 제곱합을 최소화하는 방향을 찾는 것!<br/>

&ensp;수학적으로 보면?<br/>
* n차원 -> k차원 줄이려면:
  - 서로 직교하는 k개의 벡터 (u¹, u², ..., uᵏ) 를 찾아야 한다.
  - 각각의 데이터 x를 이 k개의 축 위로 투영해서 z 벡터를 얻는다.
* 예: 3차원 벡터 (x₁, x₂, x₃) → 2차원 벡터 (z₁, z₂)

&ensp;PCA vs 선형 회귀(Linear Regression)의 차이<br/>
<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-8.png" width="600"></p>

&ensp;PCA는 "데이터를 요약"하려는 것이고 회귀는 무언가를 예측하려는 거다.<br/>

&ensp;요약<br/>
<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-9.png" width="600"></p>