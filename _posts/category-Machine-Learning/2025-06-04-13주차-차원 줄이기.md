---
title: "13주차 차원 줄이기기"
excerpt: ""

wirter: sohee kim
categories:
  - Machine Learning
tags:
  - Machine Learning

toc: true
toc_sticky: true
use_math: true
  
date: 2025-06-04
last_modified_at: 2025-06-04
---

&ensp;차원 줄이기(Dimensionality Reduction)<br/>
&ensp;현실의 데이터는 단순히 두세 개 숫자가 아니라, 수십 개의 특징(Feature) 값들로 이루어진 고차원 벡터이다.<br/>
&ensp;예를 들어:<br/>
* 자동차의 성능을 나타내는 20가지 특징이 있다면 → 데이터는 20차원 벡터로 표현됨.

&ensp;이렇게 차원이 높아지면 생기는 문제점<br/>
* 계산 시간이 길어진다! (컴퓨터가 느려짐)
* 사람이 눈으로 보기 어렵다! (그래프 표현 불가)

&ensp;그래서 데이터를 간단하게, 정보는 최대한 유지하면서 차원을 줄이는 기술이 필요하다<br/>

&ensp;차원 줄이기의 목적 2가지<br/>
<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-1.png" width="600"></p>

&ensp;데이터 압축<br/>
&ensp;데이터에는 중복된 정보(Redundancy)가 많다.<br/>

&ensp;예시 1: cm와 inch<br/>
* 어떤 물체의 길이를 cm와 inch로 저장 -> 두 값은 사실상 같은 정보
* 동일한 길이이므로 두 특징 값은 매우 높은 상관관계 유지(즉 중복)

&ensp;이런 경우, 두 개의 특징 대신 하나의 축으로 줄일 수 있다.<br/>
<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-2.png" width="600"></p>

&ensp;예시 2: 조종사의 실력(skill) vs 열정(enjoyment)<br/>
* 실력이 높아지면 비행을 더 좋아하게 되고 → 서로 상관관계 높음
* 두 개 특징 → 하나로 합쳐서 ‘적성(aptitude)’이라고 볼 수 있다.

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-3.png" width="600"></p>

&ensp;요점: 서로 비슷한 정보를 가진 특징들은 하나로 합쳐도 괜찮다.(즉 불필요한 redundancy를 줄이기 위해 2개의 매우 높은 상관관계를 보이고 있는 특징 값을 하나의 특징 값으로 변환)<br/>
&ensp;이렇게 하면 차원을 줄이면서도 정보 손실을 최소화할 수 있다. <br/>

&ensp;투영(Projection)<br/>
&ensp;고차원 데이터를 낮은 차원으로 줄일 때, 데이터들을 새로운 축(z₁, z₂ 등) 위로 “투영”시킨다. <br/>

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-4.png" width="600"></p>

&ensp;데이터 시각화<br/>
&ensp;50차원 벡터 같은 고차원 데이터는 그래프로 보기 어렵다.<br/>

&ensp;예시: 나라별 데이터<br/>
* South Korea: GDP, 인구, 기대수명, 교육지수 등… 50가지 지표!
* 이런 데이터는 50차원 벡터로 표현 -> 그래프로 보기 힘듦

&ensp;그래서 고차원을 2차원으로 줄여서 \(z1, z2\)형태로 각 나라를 점 하나로 표현 할 수 있다. <br/>

&ensp;중요한 점<br/>
* 줄인 차원의 의미 \(z1, z2\)는 해석이 어려움
* z₁, z₂는 단순히 시각화 목적이지 각각 GDP나 인구를 의미하지 않는다. 

<p align="center"><img src="/assets/img/Machine Learning/13. 차원 줄이기/13-5.png" width="600"></p>