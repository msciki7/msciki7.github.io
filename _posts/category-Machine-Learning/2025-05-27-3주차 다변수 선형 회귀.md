---
title: "3주차 다변수 선형 회귀"
excerpt: ""

wirter: sohee kim
categories:
  - Machine Learning
tags:
  - Machine Learning

toc: true
toc_sticky: true
use_math: true
  
date: 2025-05-27
last_modified_at: 2025-05-27
---


1\. 다변수 선형 회귀
======
* 목표: 여러 개의 입력 특정값(features)을 이용해 자동차 가격(Price) 예측
* 예측 입력 변수:
    - $x_1$ : 엔진 출력 (Engine Power)
    - $x_2$ : 최대 회전속도 (Peak RPM)
    - $x_3$ : 자동차 도어 개수(Number of Doors)
* 예측 함수(Hypothesis Function): $h(x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3 = \mathbf{w}^T \mathbf{x}$  

2\. 수학적 표현
======

1. 예측 변수 (Hypothesis)
* 일반화된 벡터 표현 : $h_w(x) = w^Tx$
* $w= [w_0, w_1, ..., w_n]^T$ (n+1개의 파라미터)
* $x = [1, x_1,..., x_n]^T$ (Bias를 위해 $x_0 = 1$ 포함) 

2. 비용함수(cost Function)
* 평균 제곱 오차: $J(\mathbf{w}) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_{\mathbf{w}}(x^{(i)}) - y^{(i)} \right)^2$ 

3. 파라미터 최적화 방법
&ensp; Gradient Descent (경사 하강법)<br/>
* 모든 파라미터 $w_j$ 에 대해 반복적으로 다음과 같이 업데이트: $w_j := w_j - \alpha \cdot \frac{\partial J(\mathbf{w})}{\partial w_j}$

* α: 학습률 (learning rate)
* 파라미터 벡터화 표현: $\mathbf{w} := \mathbf{w} - \alpha \cdot \nabla J(\mathbf{w})$ 

&ensp;파생된 편미분 식: $\frac{\partial J(\mathbf{w})}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} \left( h_{\mathbf{w}}(x^{(i)}) - y^{(i)} \right) \cdot x_j^{(i)}$<br/>

&ensp;파라미터 벡터화: 비용함수가 최솟값이 될 때까지 반복 

3\. 특정 값 스케일링
======

1. 왜 스케일링이 필요한가?
*  여러 개의 특징값(features)이 서로 **값의 범위(Range)**가 다를 경우,
    - 예: 엔진 파워 x1 = (100 ~ 250), Peak RPM(4000 ~ 8000)
* 이런 비균형한 스케일은 비용 함수 J(w)의 등고선이 찌그러지게(skewed contours) 만들어 경사하강법이 최솟값으로 수렴하는 속도를 느리게 하거나 잘못된 방향으로 진동하며 수렴을 방해한다.

2. 해결책: 특징 값 스케일링 방법
* 방법 1: 최댓값 기준 정규화 (Min-Max Scaling)
    - 각 feature를 해당 feature의 최댓값으로 나눔: $x_j' = \frac{x_j}{\max(x_j)}$
    - 예 :
        + 엔젠파워: $x'_1 = x_1/250$
        + RPM: $x'_2 = x_2/8000$
    - 결과적으로 각 feature의 최대값이 1이 되며 값의 범위를 비슷하게 맞출 수 있다.

* 방법 2: 평균 정규화 (Mean Normalization)
* 수식: $x_j' = s_j (x_j - \mu_j)$ 
* $μ_j$ : feature j의 평균
* $s_j$ : 동적 범위(Dynamic Range) 또는 표준편차(Standard Deviation)
* 엔진파워: $x_1' = \frac{150}{x_1 - 175}$
* RPM: $x_2' = \frac{x_2 - 6000}{4000}$ 

3. 스케일링 결과의 영향
* 학습 전에 스케일링을 적용하면 다음과 같은 효과:
    - 등고선이 원형에 가까워짐
    - 경사하강법의 수렴 속도 증가
    - 더 안정적인 학습 가능
* 스케일링은 선형 회귀뿐만 아니라 로지스틱 회귀 등 다른 경사하강법 기반 알고리즘에도 효과적임

&ensp;올바른 vs 잘못된 스케일 예시
<p align="center"><img src="/assets/img/Machine Learning/3. 다변수 선형 회귀/3-1.png" width="600"></p>

4\. 경사하강: 학습속도
======

&ensp;경사 하강(gradient desent)은 컴퓨터가 어떤 문제를 풀 때 정답에 점점 가까워지는 방법이다. 예를 들어 어떤 문제에서 점수를 가장 많이 받을 수 있는 답을 찾고 싶을 때 조금씩 바꿔보면서 더 좋은 답을 찾는 것이다. <br/>

* 학습 속도
    - 컴퓨터가 얼마나 빨리 답을 찾아가는지 나타내는 숫자이다. 이 숫자를 우리는 **학습률(learning rate)**이라고 부른다.
    - 학습률이 너무 작으면 -> 거북이처럼 천천히 가서 시간이 너무 오래 걸린다.
    - 학습률이 너무 크면 -> 점프를 너무 멀리 해서 정답을 지나쳐버린다.(길을 잃을지도...)

* 학습 곡선(Learning Curve)이라는 그래프
&ensp;컴퓨터가 문제를 풀면서 정답에 얼마나 가까워졌는지를 보여주는 그래프이다.<br/>
<p align="center"><img src="/assets/img/Machine Learning/3. 다변수 선형 회귀/3-3.png" width="600"></p>

* 그래프가 계속 아래로 내려간다 -> 잘 배우고 있다.  
<p align="center"><img src="/assets/img/Machine Learning/3. 다변수 선형 회귀/3-2.png" width="600"></p>

* 그래프가 올라간다 -> 배우는 데 문제가 있다는 뜻이다.

&ensp;멈추는 시점은 언제?<br/>
&ensp;컴퓨터가 너무 많이 반복해서 계산하면 시간이 아깝다. 그래서:
* 이전보다 거의 달라지지 않으면 -> 아 이제 다 배웠구나하고 멈춘다.
* 예: 숫자가 0.001보다 조금만 바뀌면 -> 멈추는 게 좋다.

&ensp;학습률을 고르는 방법<br/>
&ensp;학습률을 정할 때는 여러 숫자를 직접 시험해보면서 제일 좋은 걸 고르는 것이다.<br/>
&ensp;예를 들어:<br/>
* 0.1부터 시작해서
    - 너무 느리면 → 1이나 10처럼 큰 숫자로 바꿔보고
    - 너무 빨라서 망가지면 → 0.01, 0.001처럼 더 작은 숫자로 바꿔보는 것이다. 

&ensp;정리<br/>
<p align="center"><img src="/assets/img/Machine Learning/3. 다변수 선형 회귀/3-4.png" width="600"></p>


5\. 새로운 특정 값 만들기
======

&ensp;🚗 자동차 예측 문제에서 새로운 특징 만들기란?<br/>
&ensp;우리가 어떤 자동차의 가격을 맞히고 싶다고 생각해보자! 자동차에 대해 알 수 있는 정보들(=특징값)이 있다. <br/>
&ensp;예를 들어:<br/>
* 자동차의 너비(폭)
* 자동차의 길이
* 자동차의 높이
&ensp;이런 정보들을 숫자로 받아서 자동차의 가격을 예측하는 거다.<br/>

&ensp;예전 방식: 각각의 특징을 따로 사용했다.<br/>
&ensp;자동차 가격 = 어떤 수 × 너비 + 어떤 수 × 길이 + 어떤 수 × 높이 + 어떤 기본값<br/>
&ensp;이걸 수식으로 쓰면: 자동차 가격 = w₀ + w₁×너비 + w₂×길이 + w₃×높이<br/>

&ensp;새로운 아이디어: 새로운 특징 값을 만들자!<br/>
&ensp;그런데  너비 × 길이 × 높이 = 부피(volume)이다. 한번에 자동차의 크기를 보여주는 숫자이다. 그러니까 이렇게 바꿀 수 있다.<br/>
* 자동차 가격 = w₀ + w₁×부피
&ensp;3개를 따로 쓰는 것보다 하나로 만든 부피가 더 간단하고 효과적일 수 있다.<br/>

&ensp;왜 이렇게 하나로 만들까?<br/>
* 너비, 길이, 높이는 비슷한 정보야 (다 자동차의 크기!)
* 쓸데없이 비슷한 정보가 많으면 모델이 헷갈릴 수 있다.

&ensp;다항식 특징도 만들어 볼 수 있다.<br/>
&ensp;예를 들어:<br/>
* 엔진 파워 = 100일 때
    - 그냥 파워: 100
    - 제곱: 100 x 100 = 10,000
    - 세제곱: 100 x 100 x100 = 1,000,000
&ensp;이런 걸 전부 새로운 특징 값으로 쓰면 복잡한 모양의 그래프도 잘 따라갈 수 있다.<br/>
&ensp;하지만 이런 값들이 너무 커지면 학습이 잘 안 될 수도 있어서 스케일링이 꼭 필요하다.

&ensp;함수로 만든 특징 값도 있다. 
<p align="center"><img src="/assets/img/Machine Learning/3. 다변수 선형 회귀/3-5.png" width="600"></p>

6\. 정규 방정식
======

&ensp;자동차 가격을 예측할 수 있는 식(수학 공식)을 한 번에 계산해서 만드는 방법<br/>

1. 예측 함수(Hypothesis)
&ensp;자동차 가격 = w0 + w1 * 엔진파워 + w2 * 최대회전수 + w3 * 문 개수<br/>
&ensp;여기서 w0, w1, w2, w3는 우리가 구해야 하는 숫자이다. (이걸 "파라미터"라고 부른다.)<br/>

2. 데이터
&ensp;우리는 여러 자동차 정보를 가지고 있다.<br/>
&ensp;예:<br/>
<p align="center"><img src="/assets/img/Machine Learning/3. 다변수 선형 회귀/3-6.png" width="600"></p>

3. 정규 방정식으로 구하기 
&ensp;모든 데이터를 수학 공식으로 바꾸고 다음 수식을 계산하면 최적의 w0, w1, w2, w3 값을 한 번에 구할 수 있다. <br/>
&ensp;$w=(X^TX)^{-1}X^Ty$ <br/>
&ensp;이 식에서: <br/>
* X 는 자동차 스펙들(엔진 파워, 회전수, 문 수)전체
* y는 실제 자동차 가격들
* w는 우리가 찾고 싶은 숫자들(w0, w1, w2, w3)

&ensp;경사 하강법과 정규 방정식의 차이
<p align="center"><img src="/assets/img/Machine Learning/3. 다변수 선형 회귀/3-7.png" width="600"></p>

&ensp;주의할 점<br/>
* 너무 비슷한 특징값을 넣으면 안 됨(예: x1 = 엔진파워, x2 = 엔진파워x2)
* 데이터 수가 너무 적고 특정값이 너무 많으면 계산이 안 될 수도 있다.
* 이럴 땐 특징을 줄이거나 regularization(정규화) 기법을 사용하면 해결할 수 있다. 

