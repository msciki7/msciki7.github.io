---
title: "4주차 로지스틱 회귀"
excerpt: "이진 분류, 로지스틱 회귀, 특징 값 분류 경계선, 비용 함수, 최적 파라미터에 대해 다룹니다."

wirter: myeongwoo Yoon
categories:
  - Machine Learning
tags:
  - Machine Learning

toc: true
toc_sticky: true
use_math: true
  
date: 2025-05-30
last_modified_at: 2025-05-30
---

1\. 이진 분류
======

* 이진 분류(Binary Classification): 출력 값이 두 가지 클래스(0 또는 1) 중 하나인 분류 문제.
* 예시
  - 코로나 검사: 양성(1) / 음성(0)
  - 이메일 분류: 스팸(1) / 정상(0)
  - 품질 검사: 불량(1) / 정상(0)
  - 얼굴 인식: John 맞음(1) / 아님(0)
* 표기 방식
  - {0, 1}: 0 또는 1의 불연속적인 값
  - \[0, 1\]: 0과 1 사이의 연속적인 값
* 선형 회귀의 한계 (이진 분류에 적용할 경우)
  - 선형 회귀 모델은 예측값이 0~1을 벗어날 수 있음 → 확률로 해석 불가
  - **임계치(Threshold)**를 사용해 예측값이 0.5보다 크면 1, 작으면 0으로 분류 가능하지만, 이상치(Outlier) 데이터의 영향으로 모델이 왜곡될 수 있음 → **결정 경계(Decision Boundary)**가 바뀜
  - 우리가 원하는 데이터가 들어오지 않았을 경우 예측은 잘못될 수 있음

<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/1-1.png" width="600"></p>

로지스틱 회귀
------

* 로지스틱 회귀(Logistic Regression)
  - 해결 방법: 선형 회귀의 예측값을 Sigmoid 함수에 통과시켜 \[0, 1\] 범위의 확률로 해석
  <p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/1-2.png" width="600"></p>
    + $g(-\infty) = 0$
    + $g(0) = 0.5$
    + $g(\infty) = 1$
  - 예측 함수
  <p align="center">$h(x) = g(w^T x) = \frac{1}{1 + e^{-w^T x}}$</p>​
    + 여기서 $\theta$는 파라미터, $x$는 입력 특징
  - 의미
    + $h(x)$: 주어진 $x$에 대해 **양성 클래스(1)**가 될 확률
    + $1 - h(x)$: 음성 클래스(0)일 확률
  - 결정 규칙
    + $h(x) \geq 0.5$, 또는 $w^T x \geq 0$: 클래스 1(양성)
    + $h(x) < 0.5$, 또는 $w^T x < 0$: 클래스 0(음성)
    <p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/1-3.png" width="600"></p>

예시
------

<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/1-4.png" width="600"></p>

* $y = 1$일 때, 예측 함수 $h(x) = P(y = 1 \| x; w)$가 크면 양성일 가능성이 높고, 작으면 음성일 가능성이 높다.
* $h(x) = P(y = 0 \| x; w) + h(x) = P(y = 1 \| x; w) = 1$
* $h(x) = P(y = 0 \| x; w) = 1 - h(x) = P(y = 1 \| x; w)$
* 즉, $g(x) = 0.7$은 양성일 확률 70%, 음성일 확률 30%을 의미

&ensp;결론은 다음과 같다.
* 선형 회귀는 이상치에 민감하고, 예측값이 확률처럼 해석되지 않음.
* 로지스틱 회귀는 Sigmoid 함수를 통해 0~1 범위의 확률로 출력 가능하며, 이진 분류 문제에 더 적합함.

2\. 분류 경계선
======

* 분류 경계선: 로지스틱 회귀(Logistic Regression)를 사용할 때, 입력 공간에서 클래스(0 또는 1)를 나누는 경계선
  - 앞에서 $y = 0.5$ 즉, 확률이 50%일 때가 분류 경계선이고 이를 기준으로 예측값이 0.5 이상이면 클래스 1, 0.5 미만이면 클래스 0을 의미

* 앞에서 $h(x) = g(w^T x) = g(w_0 + w_1x_1 + w_2x_2)$ 

<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/2-1.png" width="600"></p>

<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/2-2.png" width="600"></p>


선형, 비선형 경계
------

<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/2-3.png" width="600"></p>

* 이때, 예측 함수 $h(x) = g(w_0 + w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_2^w)$이라고 가정
* $w_0 = -1, w_1 = 0, w_2 = 0, w_3 = 1, w_4 = 1$이라 하면, $h(x) = g(-1 + x_1^2 + x_2^2)$
* 따라서 $x_1^2 + x_2^2 = 1$이 Decision Boundary가 됨
  - 분류 경계선은 sigmoid($g(x)$)의 출력이 0.5가 되는 곳, 즉 $w^T x$가 0이 되는 지점
* 경계는 원, 곡선 등 더 복잡한 형태가 될 수 있음음

<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/2-4.png" width="600"></p>

3\. 로지스틱 회귀: 비용함수
======

&ensp;선형 회귀와 달리 오차 제곱을 사용하면 convex 함수가 보장되지 않으므로, 로지스틱 회귀에서는 **로그 손실 함수(Log Loss)**를 사용<br/>
<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/3-1.png" width="600"></p>

* $y = 1$인 경우
  - $cost(h(x), y) = - \log(h(x))$
  - $h(x)$가 1에 가까울수록 cost가 작아짐, 즉 잘 분류함
* $y = 0$인 경우
  - $cost(h(x), y) = - \log(1 - h(x))$
  - $h(x)$가 0에 가까울수록 cost가 작아짐
* 위 두 경우를 합하면 비용 함수 $cost(h(x), y) = -y\log(h(x)) - (1 - y)\log(1 - h(x))$ (Combined Cost Function)

&ensp;전체 데이터에 대한 비용함수는 다음과 같다.<br/>
<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/3-2.png" width="600"></p>

* 이 함수는 convex하므로 Gradient Descent를 통해 전역 최솟값(global minimum)에 도달 가능

<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/3-3.png" width="500"></p>

<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/3-4.png" width="500"></p>

4\. 최적화
======

&ensp;비용 함수 $J(w)$는 모델이 얼마나 잘 예측하는지를 측정하는 함수로, 우리가 학습 과정에서 최소화하고자 하는 대상이다. 즉, $w$를 조정해서 비용함수의 값을 최소화해야 한다.

경사하강법(Gradient Descent)
------

* 초기값 설정: $w$를 임의로 설정
* 반복 과정
  - 비용 함수의 gradient $\triangledown J(w)$ 계산
  - J가 최솟값에 도달할 때까지(수렴하거나 반복 횟수 초과 시) 식으로 업데이트($\alpha$: 학습률)
  <p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/4-1.png" width="500"></p>
* 경사하강법을 대체하는 다름 알고리즘도 존재
  - Conjugate Gradient
  - BFGS
  - L-BFGS
* 최적화 알고리즘의 장단점
  - 장점
    + No need to manually pick $\alpha$
    + Use different learning rate for every iteration
    + Often faster than gradient descent
  - 단점
    + More complex

예시
------

<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/4-4.png" width="600"></p>

<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/4-2.png" width="500"></p>

<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/4-3.png" width="500"></p>

* 최적화된 w = [5 3]

5\. Multicalss Classification
======

* **Multiclass Classification**: 부류(class)가 2개 이상인 분류 문제
  - 딸기 품질 분류: Regular, Premium, Super-Premium (3개 클래스)
  - 부품 검사: Normal, Dent, Scratch, Crack, Dust (5개 클래스)
  - 날씨 예측: Sunny, Cloudy, Rain, Snow (4개 클래스)

One-vs-All Classification
------

* 다중 클래스 문제를 여러 개의 이진 분류 문제로 변환하여 해결
* 각 클래스 $C_i$에 대해 이진 분류기( $h_i(x) = P(y=i\|x;w)$ )를 학습
  - 입력 x가 클래스 i일 확률을 추정하는 로지스틱 회귀 모델
* 예측
  - 모든 $h_i(x)$를 계산한 후, $h_j(x)$를 최대화하는 class i를 선택
  <p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/5-1.png" width="300"></p>

예시
------

<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/5-2.png" width="600"></p>

<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/5-3.png" width="600"></p>

<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/5-4.png" width="600"></p>

<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/5-5.png" width="600"></p>

<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/5-6.png" width="600"></p>

<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/5-7.png" width="600"></p>

&ensp;**One-vs-All Classification**을 정리하면 다음과 같다.<br/>
<p align="center"><img src="/assets/img/Machine Learning/4주차 로지스틱 회귀/5-8.png" width="600"></p>
