---
title: "5주차 정규화"
excerpt: "과적합 문제, 정규화 개념, 정규화 적용 방법에 대해 다룹니다."

wirter: myeongwoo Yoon
categories:
  - Machine Learning
tags:
  - Machine Learning

toc: true
toc_sticky: true
use_math: true
  
date: 2025-06-01
last_modified_at: 2025-06-01
---

1\. 과적합(Overfitting)
======

* 과적합(Overfitting): 학습 데이터에는 매우 잘 맞지만, 새로운 데이터(테스트 데이터)에는 일반화되지 못해 성능이 저하되는 현상.
  - 소수 샘플(예: 사이즈별 5명)만 참고하여 티셔츠를 만들었더니 일부에게만 잘 맞고 대부분에게는 맞지 않음

<p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/1-1.png" width="600"></p>

* 단순 모델: 직선은 데이터 전체의 흐름을 설명하지 못함(Undeffitting)
<p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/1-2.png" width="600"></p>

* 적절한 모델: 데이터의 경향을 잘 반영한 곡선
<p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/1-3.png" width="600"></p>

* 과도한 모델: 데이터 포인트는 정확히 지났지만, 전체적인 경향을 왜곡(Overfitting)
<p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/1-4.png" width="600"></p>

과소적합(Underfitting)과의 비교
------

* Overfitting: 모델이 너무 복잡해서 학습 데이터에는 잘 맞지만, 새로운 데이터에 일반화되지 않음(High Variance)
* Underfitting: 모델이 너무 단순해서 학습 데이터도 제대로 설명하지 못함(High Bias)

일반화(Generalization) 문제
------

* 학습 데이터에 대해서는 높은 성능을 보이지만, 새로운 데이터에는 성능 저하가 발생하는 문제
* 과적합된 모델은 학습 오차는 작으나, 테스트 오차가 큼큼

과적합을 피하는 방법
------

* 특징 수 감소(Reduce Number of Features)
  - 중요한 feature만 선택하거나, 모델 선택 알고리즘 사용
* 학습 데이터 수 증가(Increase Number of Training)
  - 학습 샘플을 많이 확보하면 과적합이 줄어듦
* 정규화(Regularization)
  - 모든 feature를 사용하되 각 가중치의 크기를 줄이는 방식
  - Keep all the features, but reduce magnitude/values of parametes $w_j$
  - Works well with many features, each of which contributes a bit to predicting $y$

2\. 정규화(Regularization)
======

* 정규화(Regularization): 모델의 복잡도를 조절하여 과적합(Overfitting)을 방지하는 기법
  - 모든 파라미터를 완전히 제거하지 않고 파라미터의 값을 작게 만드는 방향으로 학습을 유도합니다.

<p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/2-1.png" width="600"></p>

* 고차 다항식은 학습 데이터에는 완벽하게 맞지만, 테스트 데이터에는 일반화되지 않음
* 정규화를 사용해 $w_3, w_4$의 값을 작게 만들고 모델을 단순화시켜 일반화 성능을 향상시킬 수 있음

정규화된 비용 함수
------

* Cost Function & Regularized Cost Function
<p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/2-2.png" width="600"></p>

* Regularized Cost Function
  - $\lambda$ (Regularization parameter): 정규화의 강도를 조절하는 하이퍼파라미터
  - 주의: $w_0$(bias term)은 정규화에 포함되지 않음

&ensp;정규화된 Cost Funciton은 두 가지 목표를 동시에 수행
1. 학습 데이터에 잘 맞도록 하는 것
2. 파라미터의 크기를 작게 유지하는 것

<p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/2-3.png" width="600"></p>

예제
------

<p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/2-4.png" width="600"></p>

* $w_3, w_4$를 작게 만들어서 Higher-order Terms에 페널티를 줄 수 있음
* $\lambda = 1000$이라고 하면, 비용 함수는 $w_3, w_4$가 조금만 커져도 전체 비용이 급격히 커짐.
* 비용함수를 될 수 있으면 작게 유지시키려면, 페널티 항을 아주 작게 만들어야 됨
  - $w_3, w_4$의 값을 굉장히 작게 만들어야 됨
* 두 파라미터 값을 굉장히 작게 만들면 모델의 복잡도가 줄어들어 Overfitting 문제를 줄일 수 있음
* 즉, $w_3 \approx 0, w_4 \approx 0$

<p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/2-5.png" width="600"></p>

* 만약 $\lambda = 10^10$이라면, 모든 파라미터들이 커지게 되므로 $w_1 \approx 0, w_2 \approx 0, w_3 \approx 0, w_4 \approx 0$, $h(x) = w_0$가 됨
* 수평선 모델이므로 underfitting(High Bias)

&ensp;즉, $\lambda$에 대해 정리하면 다음과 같다.
* $\lambda = 0$: 정규화가 없으므로 과적합 위험
* $\lambda$ 작음: 학습 데이터 적합 중시
* $\lambda$ 큼: 모델 단순화 중시, underfitting 가능성

3\. 선형 회귀의 정규화
======

&ensp;과적합을 방지하기 위해 정규화가 필요하다. 특징(feature) 개수가 학습 데이터 수보다 많을 때 일반화 성능이 떨어지는 문제를 해결할 수 있다.

선형 회귀의 기본 구조
------

* Hypothesis: $h(x) = w_0 +w_1x_1 + w_2x_2 + \dots + w_nx_n = w^T x$
  - $w = \[ w_0, w_1, \dots, w_n \]$, $x = \[1, x_1, \dots, x_n \]$
* Cost Function
<p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/2-2.png" width="600"></p>

정규화가 적용된 알고리즘
------

<p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/3-1.png" width="600"></p>

* Gradient descent
  - Gradient descent to find optimal parameters
  - Repeat until J reaches its minimum
  <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/3-2.png" width="600"></p>
  - Regularized gradient descent includes $w_j(1 - \alpha\frac{\lambda}{m})$
  <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/3-3.png" width="600"></p>

* Normal equation
  - Normal equation to find optimal parametes
  <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/3-4.png" width="600"></p>
  <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/3-5.png" width="600"></p>
  <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/3-6.png" width="600"></p>
  <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/3-7.png" width="600"></p>

Non-Invertibility 문제 해결
------

* **"역행렬이 존재하는가, 존재하지 않는가"**
  - $m \lea n$ 인 경우, $X^T X$는 singular이므로 역행렬이 불가능
  - 정규화 항 $\lambda L$을 더하면 역행렬이 항상 존재함

<p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/3-8.png" width="600"></p>


4\. 로지스틱 회귀의 정규화
======

정규화의 필요성
------

* 높은 차수의 다항식은 복잡한 분류 경계선을 가지므로 **과적합(overfitting)**을 유발
* 이를 방지하기 위해 **파라미터 크기를 제어**할 수 있는 정규화 항을 추가

정규화가 적용된 Cost Function
------

* Cost Function(No Regularization)
  <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/4-1.png" width="600"></p>
* Regularized Cost Function
  <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/4-2.png" width="600"></p>

Regularization Implementation(구현)
------

* Gradient Descent
  - Sigmoid 
  <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/4-3.png" width="500"></p>
  - update $w_0$
  <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/4-4.png" width="500"></p>
  - update $w_1$ (Same for $w_2, \dots, w_n$)
  <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/4-5.png" width="500"></p>
  - 정규화 효과
    + 정규화 X: 과도한 $w$값으로 복잡한 경계선을 가지므로 Overfitting
    + 정규화 O: 작은 $w$값을 유지하므로 단순한 경계선을 가지므로 일반화 성능이 올라감
* Advanced Optimization(최적화 함수 사용)
  - fminuc() 함수를 사용해 cost와 gradient를 return
    + compute J
    <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/4-6.png" width="600"></p>
    + compute gradient
    <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/4-7.png" width="500"></p>
    <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/4-8.png" width="500"></p>

5\. 다항(Polynomial) 회귀의 정규화
======

A Data Generation Model
------

<p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/5-1.png" width="600"></p>

* 예측 목표: $t \approx \sin(2\pi x)$
* 데이터: 관측지는 노이즈가 포함된 $t$값
* M차 다항식(M-th order polynomial)
<p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/5-2.png" width="600"></p>

Cost function
------

* Strategy: The values of the coefficients will be determined by fitting the polynomial to the training date
* Cost function - 기본 오차 함수(SSE) 사용
  <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/5-3.png" width="600"></p>
  - parameter Optimization
  <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/5-4.png" width="600"></p>

Curve Fitting 시각화
------

<p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/5-5.png" width="500"></p>

<p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/5-6.png" width="600"></p>

* $M = 0$(상수 함수): Underfitting
* $M = 1$(선형 함수): 단순한 패턴 가능
* $M = 3$(적절한 모델): 예측 잘 수행
* $M = 9$(고차 다항식): Overfitting
  - M이 커질수록 데이터에 과도하게 맞추려는 경향(오차 0)이 있지만, 일반화 성능이 떨어짐짐

일반화
------

* 오차 비교 지표: RMS
  <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/5-7.png" width="600"></p>
  - Training error은 계속 감소
  - Test error는 어느 시점에서 증가하므로 Overfitting 확인 가능
* Reduce Overfitting
  - Increase the size of training set
    + 데이터 수가 적을수록 고차 다항식은 과적합에 취약
    + 데이터 수가 많아질수록 일반화 성능 증가
    <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/5-8.png" width="600"></p>
  - Regularization
    + 정규화 효과: 큰 $w$값에 페널티를 부여해 $w$를 작게 만들어 고차항의 영향을 줄여, 과적합을 방지
    <p align="center"><img src="/assets/img/Machine Learning/5주차 정규화/5-9.png" width="600"></p>

&ensp;정리하면, 함수의 예측 문제에서 정규화 파라미터의 값을 이용하여 정규화를 어느 정도 적용할 수 있을지 조절할 수 있다.