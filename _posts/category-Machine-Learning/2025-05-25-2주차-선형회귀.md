---
title: "2주차 선형회귀"
excerpt: "선형 회귀 모델링, 비용 함수, 경사 하강에 대해 다룹니다."

wirter: Myeongwoo Yoon
categories:
  - Machine Learning
tags:
  - Machine Learning

toc: true
toc_sticky: true
use_math: true
  
date: 2025-05-25
last_modified_at: 2025-05-25
---

1\. 선형 회귀: 모델 표현
======

* 한 기업이 **온라인 광고비(X)**를 증가시키면 **제품 매출액(Y)**이 증가하는 패턴을 발견.
* 향후 **광고비 지출에 따른 매출 예측 모델**을 만들기 위해 선형 회귀(linear regression) 기법을 활용.

&ensp;**선형 회귀(Linear Regression)**은 입력값과 출력값 간의 관계를 직선의 방정식으로 모델링하여, 보지 못한 새로운 입력값에 대해 출력값을 예측하는 지도학습 방법이다.
* $h(x) = w_0 + w_1x$
  - $h(x)$: 예측값
  - $x$: 입력값(예, 엔진 파워, 광고비)
  - $w_0$: y절편(bias)
  - $w_1$: 기울기(slope)

자동차 가격 예측 예제
------

* 예제 구성
  - 입력 (x): 엔진 파워 (horsepower)
  - 출력 (y): 자동차 가격 ($)
* 예측 목표:
  - 115 마력의 자동차 가격을 예측 → 예측값: 약 $2,300
* 학습 데이터 예시
<p align="center"><img src="/assets/img/Machine Learning/2주차 선형 회귀/1-1.png" width="600"></p>

* 모델 학습
  - 입력-출력 쌍($(x^{(i)}, y^{(i)})$) 형태로 총 m = 205개 샘플을 기반으로 학습

선형 회귀 모델의 구조
------

* 예측 함수 h(x)
  - 1차 함수
  - 파라미터 $w_0, w_1$에 따라 직선의 위치와 기울기 결정
* 파라미터 의미
  - $w_0$: y축과의 절편 (bias)
  - $w_1$: 기울기 (x의 변화에 따른 y 변화량)

|예시|식|의미|
|---|---|---|
|$w_0=1.5, w_1 = 0$| $h(x) = 1.5$| 수평선(절평만 있음)|
|$w_0=0, w_1 = 0.5$| $h(x) = 0.5x$| 원점을 지나는 기울기 0.5|
|$w_0=1, w_1 = 0.5$| $h(x) = 1 + 0.5x$| 기울기 0.5, 절편 1|


학습 목표 - 파라미터 학습
------

* 최종 목표: $\min_{w_0, w_1} \sum_{i=1}^{m} \left( h(x^{(i)}) - y^{(i)} \right)^2$
  - 모든 샘플에 대해 예측 오차의 제곱합을 최소화하는 $w_0, w_1$을 찾는 것
* 예측 오차(error)
  - $오차 = y^{(i)} - h(x^{(i)})$
* 좋은 예측함수: 데이터 포인트와 직선의 거리가 작음
* 나쁜 예측함수: 오차가 큼 → 예측 신뢰도 낮음

선형 회귀 모델 요약
------

* 🎯 목표: 주어진 입력에 대해 연속적인 출력 예측
* 📊 입력: 수치형 변수 x (예: 광고비, 엔진 파워 등)
* 📈 출력: 연속형 변수 y (예: 매출액, 가격 등)
* 🧮 모델: h(x)=w0​ + w1​x
* 🧠 학습 방식: 오차 제곱합 최소화 (최소제곱법)
* ✅ 분류: **지도학습(Supervised Learning)** – 라벨이 있는 데이터 사용


2\. 선형 회귀: 비용 함수
======

&ensp;**비용 함수(Cost Function)**는 모델의 **예측값과 실제 값 간의 차이(오차)**를 수치화하여,얼마나 잘 학습되었는지를 평가하는 함수입니다. 목표는 **이 비용 함수의 값을 최소화하는 파라미터를 찾는 것, 즉 최적의 직선 결정**이다.
* 예측 오차(prediction error)
  - $e_i = h(x^{(i)}) - y^{(i)}$ : 예측 함수가 실제로 가지고 있는 실제 출력과 우리가 목표로 하고 있는 목푯값하고의 차이
  - $h(x^{(i)})$: $i$번째 샘플의 예측값
  - $y^{(i)}$: $i$번째 샘플의 실제값
* 오차 제곱합(Sum of Squared Errors, SSE)

<p align="center">$SSE = \sum_{i=1}^{m} e^2_i = \sum_{i=1}^{m} \left( h(x^{(i)}) - y^{(i)} \right)^2$</p>

* 평균 제곱 오차(Mean Squared Error, MSE)
  - 재곱 오차의 합을 전체 데이터의 개수인 m으로 나눠줌

<p align="center">$MSE = \frac{1}{m} \sum_{i=1}^{m} e^2_i = \frac{1}{m} \sum_{i=1}^{m} \left( h(x^{(i)}) - y^{(i)} \right)^2$</p>

* Cost function

<p align="center">$J = J(w_0, w_1) = \frac{1}{2m} \sum_{i=1}^{m} \left( w_0 + w_1x^{(i)} - y^{(i)} \right)^2$</p>

  - $m$: 데이터 샘플 개수
  - $w_0, w_1$: 파라미터

비용 함수와 최적화 문제
------

* $(w_0^*, w_1^*) = \arg\min_{w_0, w_1} J(w_0, w_1)$
* $\mathbf{w}^* = \arg\min_{\mathbf{w}} J(\mathbf{w})$

&ensp;벡터 w에 관해서 비용 함수 J를 최소화하여 그때 얻을 수 었는 최적 파라키터를 $w^*$ 라고 한다.

비용 함수 직관 - 예제(단일 파라미터)
------

* 입력 x = {1, 2, 3}, 출력 y = {1, 2, 3} (완벽한 직선)
* 모델: $h(x)=w1 x$ (단순화 위해 $w_0 = 0$가정)

<p align="center"><img src="/assets/img/Machine Learning/2주차 선형 회귀/2-1.png" width="600"></p>

* 결론: 비용 함수 값이 작을수록 더 좋은 예측 함수임

비용 함수의 형태
------

* 단일 파라미터: 이차 함수(포물선)
  - 최솟값에서 최적 파라미터 존재
* 두 파라미터 (w₀, w₁)
  - 3차원 보울(bowl) 형태의 곡면

등고선 그래프
------

* 같은 비용 함수 값을 가지는 점들로 구성된 등고선 형태
* 중심에 가까울수록 좋은 예측 함수
* 동일한 비용 값이라도 w₀, w₁ 조합이 다르면 → 다른 예측 함수

3\. 경사 하강
======

&ensp;**경사 하강법(Gradient Descent)**는 비용 함수 $J(w_0, w_1)$를 **최소화**하는 파라미터 $w_0, w_1$을 찾는 것이다.
* 기본 아이디어
1. 임의의 초기값에서 시작
2. 비용 함수의 **기울기(경사, gradient)**를 계산
3. 그 경사의 반대 방향으로 파라미터를 업데이트
4. 최솟값에 도달하면 종료

&ensp;**경사 하강법은 비용 함수의 기울기를 따라 파라미터를 반복적으로 조정하여 최적값을 찾는 대표적인 최적화 알고리즘입니다.**

수학적 정의
------

<p align="center"><img src="/assets/img/Machine Learning/2주차 선형 회귀/3-1.png" width="600"></p>

<p align="center"><img src="/assets/img/Machine Learning/2주차 선형 회귀/3-4.png" width="600"></p>

<p align="center"><img src="/assets/img/Machine Learning/2주차 선형 회귀/3-2.png" width="600"></p>

<p align="center"><img src="/assets/img/Machine Learning/2주차 선형 회귀/3-3.png" width="600"></p>

* 단계적으로 초기값으로부터 우리가 원하는 최적 파라미터값으로 수렴시켜 나감

비용 함수 시각화
------

* Contour plot

<p align="center"><img src="/assets/img/Machine Learning/2주차 선형 회귀/3-8.png" width="600"></p>

* 등고선 중심 = 비용 함수의 최솟값 위치
* 경사 하강법은 이 중심을 향해 점점 다가감
* 비용이 낮아질수록 등고선 중심에 가까워짐

* Bowl-shaped 3D plot

<p align="center"><img src="/assets/img/Machine Learning/2주차 선형 회귀/3-6.png" width="600"></p>

* 2개의 파라미터에 대해 곡면 형태로 표현
* 최솟값은 그릇의 바닥


전역, 국소 최소값
------

* 전역 최솟값(Global Minimum): 전체 범위에서 가장 작은 비용 함수값
* 국소 최솟값(Local Minimum): 주변보다 작은 값이지만 전체 중 최솟값은 아님
* 문제점: 초기값에 따라 전역이 아닌 국소 최솟값에 수렴할 수 있음

<p align="center"><img src="/assets/img/Machine Learning/2주차 선형 회귀/3-5-지역 최솟값과 전역 최솟값.png" width="400"></p>

구현 시 주의 사항
------

* 학습률 $\alpha$
  - 너무 작으면 수렴 속도 느림
  - 너무 크면 발산하거나 진동

<p align="center"><img src="/assets/img/Machine Learning/2주차 선형 회귀/3-7-SGD가 1D 손실 함수의 값을 낮춘다.png" width="400"></p>

4\. 경사 하강 개념
======

&ensp;Gradient Descent (경사 하강법): 비용 함수 J(w)를 최소화하기 위해, 기울기의 반대 방향으로 파라미터를 반복적으로 갱신하는 최적화 알고리즘

* **경사 하강법은 기울기의 부호에 따라 항상 최솟값 방향으로 수렴하며, 학습률과 초기값에 따라 수렴 속도와 위치가 결정됩니다.**

경사의 부호와 수렴 방향
------

&ensp;경사 하강의 핵심은 경사의 부호에 따라 **항상 비용이 줄어드는 방향**으로 이동하는 것이다.

<p align="center"><img src="/assets/img/Machine Learning/2주차 선형 회귀/4-1.png" width="600"></p>

<p align="center"><img src="/assets/img/Machine Learning/2주차 선형 회귀/4-2.png" width="600"></p>

* 경사값이 양수든 음수든 항상 **최솟값 방향**으로 수렴

학습률 $\alpha$의 영향
------

* 너무 작음: 수렴은 되지만 속도가 매우 느림
* 너무 큼: Overshooting 발생 → 발산 (Divergence) 가능성

<p align="center"><img src="/assets/img/Machine Learning/2주차 선형 회귀/4-3.png" width="600"></p>

<p align="center"><img src="/assets/img/Machine Learning/2주차 선형 회귀/4-4.png" width="600"></p>

* 따라서 적절한 학습 상수를 구하는 걱이 중요함

국소 최솟값(Local Min)과 수렴
------

* 경사 하강법은 국소 최솟값에도 수렴 가능함
  - 이유: 경사값이 점점 줄어들어 도함수가 0에 가까워지면 더 이상 갱신이 안 일어나기 때문
* 고정된 학습률이라도 경사값이 작아지면서 갱신폭도 작아져서 천천히 수렴

<p align="center"><img src="/assets/img/Machine Learning/2주차 선형 회귀/4-5.png" width="600"></p>

<p align="center"><img src="/assets/img/Machine Learning/2주차 선형 회귀/4-6.png" width="600"></p>

5\. 경사 하강과 선형 회귀
======

