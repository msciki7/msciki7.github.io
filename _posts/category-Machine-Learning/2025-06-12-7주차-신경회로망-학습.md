---
title: "7주차 신경회로망 학습"
excerpt: "신경회로망, 비용 함수, 오차 역전파 학습에 대해 다룹니다."

wirter: myeongwoo Yoon
categories:
  - Machine Learning
tags:
  - Machine Learning

toc: true
toc_sticky: true
use_math: true
  
date: 2025-06-08
last_modified_at: 2025-06-12
---

비용 함수
======

&ensp;학습 알고리즘을 구현은 다음과 같이 진행하였다.
1. 비용함수 정의
2. 비용함수를 표현하고 있는 파라미터 최적화
3. 예측 함수 구현

&ensp;**입력층, 은닉층(Hidden Layer), 출력층**으로 구성된 신경회로망도 동일하게 진행한다.
* 신경회로망의 비용함수(Cost function): Measures the error between the actual output of a neural network and the desired output
  - 신경회로망의 실제 출력과 우리가 원하는 출력과의 차이를 계산한 다음 그 **오차를 최소화 하는 방향**으로 학습을 진행
* 학습(Learning): Adjusting the parameters of a neural network given a training data set
  - 비용함수를 정의한 다음 그 비용함수를 파라미터에 관해서 최소화함으로써 최적 파라미터를 구하고, 이 **최적 파라미터를 구하는 과정**
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/1-1.png" width="600"></p>
* Training samples
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/1-2.png" width="600"></p>
* Network structure
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/1-3.png" width="600"></p>
  - $L$: 입력층을 포함한 전체 층의 개수
  - $x_k$: Input layer
  - 주황색 원: Hidden layer
  - 초록색 원: Output layer(= L)
    + $y_k$: $k$번째 출력 뉴런의 값
  - $S_l$: 각 층 $l$의 bias unit을 제회한 unit의 수
    + $x_0 = 1$: 입력층의 bias unit
    <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/1-4.png" width="600"></p>

Classification
------

* Binary classification(이진 분류)
  - 출력층 뉴런 수 : 1개
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/1-5.png" width="600"></p>
* Multi-class classification(다중 분류, $K$ classes)
  - 출력층 뉴런 수: K개
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/1-6.png" width="600"></p>

&ensp;즉, 이진 분류는 1개의 출력노드, 다중 분류는 클래스 수만큼 노드가 필요함

Multilayer Feedforward Networks
------

* Multilayer: 하나 이상의 Hidden layer를 포함하고 있는 네트워크 구조
* Feedforward: **입력 신호 $\rightarrow$ Hidden layer $\rightarrow$ Output layer $\rightarrow$ 출력** 방향으로, 신호가 한 방향으로만 이동한다는 의미

<p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/1-7.png" width="600"></p>

* Input Layer
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/1-8.png" width="600"></p>
* Hidden Layer
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/1-9.png" width="600"></p>
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/1-10.png" width="600"></p>
* Output Layer
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/1-11.png" width="600"></p>
  - 이후 최종 출력값과 Desired Output과의 차이를 계산
* Network output
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/1-12.png" width="600"></p>
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/1-13.png" width="600"></p>

Error
------

* 출력층 오차는 원하는 출력과 실제 출력의 차이로 계산
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/1-14.png" width="600"></p>

Cost Function
------

* Sum of square error(오차 제곱의 합): 이 값을 최소로 하는 파라미터 계산
  - 얼마나 오차가 작게 잘 표현하고 있는지 평가하는 척도
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/1-15.png" width="600"></p>

오차 역전파 학습 알고리즘
======

* 오차 역전파(Backpropagation, BP): 신경회로망의 학습 알고리즘으로, 출력층의 오차를 기준으로 가중치를 업데이터하며, 오차를 입력층 방향으로 전달함
* 학습 과정 단계
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/2-1.png" width="600"></p>
  - Forward Pass: 입력 $\rightarrow$ 은닉층 $\rightarrow$ 출력
    + 입력 신호를 은닉층을 통해 전달하며 각 뉴런의 activation 계산
    + 출력층에서 최종 출력값 계산(예, sigmoid 함수 사용)
  - Backward Propagation: 오차 $\rightarrow$ 출력층 $\rightarrow$ 은닉층
    + 출력값과 목표값의 차이 $\rightarrow$ 오차 계산
    + 출력층과 은닉층의 **가중치(Weight)**를 각각 업데이트
* 학습 반복 단위
  - 모든 학습 샘플에 대해서 동일한 두 가지 단계(Forward Pass와 Backward Propagation)를 계속 반복시킴
  - 학습 과정을 모든 학습 데이터에 전부 적용하는 epoch를 거침
  - 여러 개의 epoch를 무수히 반복함으로써 단계적으로 신경회로망의 가중치 파라미터를 업데이트해 나감
* Cost function(오차 제곱합, Sum of Squared Errors)
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/2-2.png" width="600"></p>
  - Output layer
    <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/2-3.png" width="600"></p>
    <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/2-4.png" width="600"></p>
    + $z_j$: Hidden layer의 activation
  - Hidden layer
    <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/2-5.png" width="600"></p>
    <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/2-6.png" width="600"></p>
    + $x_i$: Input layer의 activation
  - 두 과정 모두 $\delta$가 가장 핵심적인 역할을 함. 이것을 어떻게 계속 발전시켜 나가는지가 Backpropagation 알고리즘의 핵심 사항(이를 구하는 과정은 강의록에)

$\delta$의 전파의 원리(오차 역전파)
------

* 출력층에서 계산된 오차 $\delta_k$는 은닉층으로 전달되어 은닉층의 $\delta_j$ 계산에 사용
* 은닉층의 $\delta_j$는 다시 그 이전 층으로 전파됨
* 이처럼 오차가 역방향으로 전파되며 가중치를 점진적으로 업데이트

신경회로망 파라미터 최적화
======

<p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/3-1.png" width="600"></p>

* Step 1: Forward pass
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/3-2.png" width="600"></p>
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/3-3.png" width="600"></p>
* Step 2(Backward Pass): Compute the delta of the output layer
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/3-4.png" width="600"></p>
* Step 3: Upate the weights $w^{(3)}$
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/3-5.png" width="600"></p>
* Step 4: Backpropagate the delta to layer 3
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/3-6.png" width="600"></p>
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/3-7.png" width="600"></p>
* Step 5: Update the weights $w^{(2)}$
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/3-8.png" width="600"></p>
* Step 6: Backpropagate the delta to layer 2
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/3-9.png" width="600"></p>
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/3-10.png" width="600"></p>
* Step 7: Update the weights $w^{(1)}$
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/3-11.png" width="600"></p>

&ensp;위 과정을 정리하면 다음과 같다.<br/>
<p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/3-12.png" width="600"></p>

1. 가중치 $W$ 초기화
2. 모든 학습 데이터에 대해 다음 반복 수행
  - Forward pass로 출력값 계산: 입력 신호를 계속 순방향으로 전파시켜 최종 출력을 구하는 과정
  - 오차 계산
  - $\delta$ 계산 후 Backpropagation으로 전달: 역방향으로 오차를 전파시켜 나가면서 가중치를 업데이트하는 과정
  - 모든 가중치 업데이트
3. 위 과정을 **모든 학습 데이터에 대해 한 번 반복하면 1 epoch**
4. Epoch를 여러 번 반복하며 학습

오차 역전파 알고리즘의 행렬 표현
======

* Forward Pass(순전파)
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/4-1.png" width="600"></p>
* Backward Pass(역전파)
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/4-2.png" width="600"></p>

Sigmoid 함수와 도함수
------

<p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/4-3.png" width="600"></p>

<p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/4-4.png" width="600"></p>

행렬 표현의 예
------

<p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/4-5.png" width="600"></p>

<p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/4-6.png" width="600"></p>

* 행렬 방식으로 가중치 업데이트
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/4-7.png" width="600"></p>

학습 종료 조건(Stopping Criteria)
------

* Gradient threshold
  - gradient의 크기가 충분히 작아지면 종료
  - 느리게 수렴할 수 있음
* Average error measure
  - Epoch 단위로 평균 오차 감소량이 일정 수준 이하이면 종료
  - 너무 일찍 멈출 수 있음

가중치 초기화(Weight Initialization)
------

* Initializing Weights: Weight 업데이트를 시작하기 전에 가중치 값을 초기화해야 함
  - 랜덤하게 초기화 하고 일반적으로 **Uniform 분포**를 갖도록 함
  - 양과 음의 weights가 엇비슷하게
  - 은닉층의 출력이 적당한 범위(예를 들어서 -1 에서 1 사이의 값)에 들어가도록 설정
* Sequential Update
  - Weight 업데이트가 일어나는 과정을 하나씩 진행
  - 근본적으로 랜덤한 성향이 있으므로, 수렴하는 데 좀 왔다 갔다 할 수 있는 문제가 있음
  - 데이터의 충분한 중복성이 있으므로 평균적으로 업데이트가 잘 일어남
  - 구현하기가 매우 간단해서 매우 널리 사용
  - 계산량이 단순하여 계산 속도가 빠르고, Batch Update 보다 좀 더 계산적으로 빠르기 때문에 널리 선호되고 있는 방식
* Batch Update
  - Weight 업데이트를 한꺼번에 모아서 진행행
  - 한 epoch가 끝날 때마다 한 번씩만 Weight를 업데이트하는 방식
  - 랜덤한 성향은 없어지므로 local minimun에 수렴하는 것이 보장
  - 비교적 local storage space가 많이 필요하므로 계산량이 좀 더 많고 계산속도가 느림

오차 역전파 알고리즘 응용
======

<p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/5-1.png" width="600"></p>

* 목표: 손글씨 숫자 이미지(MNIST)를 보고 0 ~ 9 중 어떤 숫자인지 분류
* 데이터셋
  - 학습용: 60000개
  - 테스트용: 10000개
  - 각 이미지: 28 X 28 픽셀, grayscale 이미지(총 784 차원의 입력 벡터)

<p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/5-2.png" width="600"></p>

* 입력층: 784노드
* 은닉층: 2개(각 10 노드)
* 출력층: 10개 노드(0 ~ 9에 대응)

&ensp;이를 학습시키면 학습이 진행됨에 따라(epoch가 진행되며) 학습 오차가 서서히 줄어든다. 데이터 수가 많아질수록 하나의 epoch를 완성하는 시간이 더 오래 걸릴 수 있다.

* 예시 결과
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/5-3.png" width="600"></p>
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/5-4.png" width="600"></p>
  <p align="center"><img src="/assets/img/Machine Learning/7주차 신경회로망 학습/5-5.png" width="600"></p>